{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "176398ca",
   "metadata": {},
   "source": [
    "# Data Science for Case Law:  Automatic Citation Treatment Analysis\n",
    "\n",
    "\n",
    "### Project authors\n",
    "\n",
    "- [Ramanathan Parameshwaran](rap940@g.harvard.edu)\n",
    "- [Malla Reddy Adaboina](maa0192@g.harvard.edu)\n",
    "- [Paul Tembo](paultembo@g.harvard.edu)\n",
    "- [Christy Chou](chrichou33@gmail.com)\n",
    "\n",
    "# Introduction\n",
    "\n",
    "## i. About this notebook\n",
    "\n",
    "\n",
    "\n",
    "### i.i. A note about supporting notebooks\n",
    "\n",
    "\n",
    "\n",
    "### i.ii. A note about supporting custom python modules\n",
    "\n",
    "\n",
    "\n",
    "## ii. Problem Statement\n",
    "\n",
    "After initial exploration and cleansing of the available data, we have focused our efforts on the following problem statement:\n",
    "\n",
    "- Use NLP to predict relevant citations based on case facts - headnotes and opinion text\n",
    "\n",
    "\n",
    "\n",
    "## iii. Summary of findings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84143268",
   "metadata": {},
   "source": [
    "# Notebook contents\n",
    "\n",
    "[Introduction](#Introduction)\n",
    "\n",
    "[Setup and Imports](#setup-and-imports)\n",
    "\n",
    "**[1. Data Description](#1.-data-description)**\n",
    "\n",
    "**[2. Data Loading and Clean-up](#2.-Data-loading-cleanup)**\n",
    "\n",
    "**[3. Garaph Analysis](#3.-graph-analysis)**\n",
    "\n",
    "**[4. Textdata Analysis](#3.-text-analysis)**\n",
    "\n",
    "**[5. Models](#3.-Models)**\n",
    "\n",
    "- [5.1. Baseline Model](#5.1.-baseline-model)\n",
    "\n",
    "- [5.2. Spectre Pooled Model](#5.2.-spectre-pooled-model)\n",
    "\n",
    "- [5.3 Legal BERT Model ](#5.3-legal-bert-model)\n",
    "\n",
    "**[6. Results and Discussions](#3.-Results-and-discussions)**\n",
    "\n",
    "**[6. Conclusions](#6.-Conclusions)**\n",
    "\n",
    "**[7. Next steps & future work](#7.-Next-steps-&-future-work)**\n",
    "\n",
    "**[8. References](#7.-Next-steps-&-future-work)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9cee20",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f160e",
   "metadata": {},
   "source": [
    "Central to winning a case before the United States court system one must put forth a compelling argument(s), the key driver of these arguments are court briefs or memos . The Memo is the primary tool  to deliver one case as to why the courts should side with them in the matter before the court. \n",
    "\n",
    "As such the parties before the court must strive to make sure they make their best arguments through the memos and briefs. One key element is not to make sure that the appropriate case law is cited in a manner that is relevant but also allows the judge(s) to find the case fairly easily.  The goal is to through legal citation provide the judges with the most relevant case information to help them make decisions as well as reference that material in a precise manner (Cornell,2021).  The reason for this is that the memo reading stage is when justices get the best chance to examine the argument being put forward , in fact some Justices like Associate Supreme Court Justice Clarence Thomas are famed for focussing on the briefs as the primary tool for making their decision which way to cast their vote (New York Times , 2015) . Others have argued that justices sometimes use the memos as templates for the opinions they write when deciding the cases. Also, some justices have noted that memos are almost always the full 50 pages length in the supreme court and that they are pleasantly surprised when they see a 35 page memo, this trend to always use up the full 50 recommended pages limit is a sign that maybe folks are pulling any and every case that is remotely related to their case and hoping that one or more of those cases resonates with the judges.All these reasons set forth above form our study question - can we use Natural Language processing to help make this lawyers generate law case citations and as such present their best case by citing the cases that give them the best chance of winning the case?\n",
    "\n",
    "Our goal here is to use Natural Language processing (NLP henceforth) to predict  which cases are most relevant to a case based on a limited set of text that represents the facts of the case. Natural language processing is a field that combines linguistics and computer science that gives us the tools that allows us to take text as input and generate output text in a desired format. In our specific case we will take in case relevant text and generate a list of law cases with their case name, court , jurisdiction etc ,with the most relevant cases listed at the top.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940f5b7a",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f10fd79",
   "metadata": {},
   "source": [
    "To install the conda virtual environment of this project use the following commands.\n",
    "\n",
    "```\n",
    "!conda env create -f environment.yml\n",
    "!conda activate cs109b-grp9-final-project\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cfc895e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bebop/anaconda3/envs/cs109b-grp9-final-project/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import tqdm\n",
    "\n",
    "import zipfile\n",
    "import lzma\n",
    "import json\n",
    "\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import SparseMatrixSimilarity\n",
    "import numpy as np\n",
    "\n",
    "from py2neo import Graph\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c026f2a",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "\n",
    "> CAP includes all official, book-published United States case law â€” every volume designated as an official report of decisions by a court within the United States.\n",
    ">\n",
    ">Each volume has been converted into structured, case-level data broken out by majority and dissenting opinion, with >human-checked metadata for party names, docket number, citation, and date.\n",
    ">\n",
    "> -- <cite>[Case.law][1]</cite>\n",
    "\n",
    "In the scope of this analysis we use the latest open case text from from bulk data exxports of open jurisdictions in the dataset. This subset consists of caselaws and metadata for the following four Jurisdictions:\n",
    "\n",
    "1. ark - Arkansas\n",
    "2. ill - Illinois\n",
    "3. nc - North Carolina\n",
    "4. nm - New Mexico\n",
    "\n",
    "These were downloaded from [here][2].\n",
    "\n",
    "\n",
    "**Citation Graph**\n",
    "\n",
    "In addition to the case text we also use the case citation graph that links extracted, verified and unambiguous  citations to cases within the dataset. We use the citation graph from [2021-04-20][3] for this study. The citation graph is present in the form of a edgelist linking one case_id to multiple case_ids. It also contains all the references to all nodes from the case.law dataset. We truncate the graph based on case_ids we found in the case texts for each jurisdiction during preprocessing.\n",
    "\n",
    "[1]: https://case.law/about/\n",
    "[2]: https://case.law/download/bulk_exports/latest/by_jurisdiction/case_text_open/\n",
    "[3]: https://case.law/download/citation_graph/2021-04-20/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f23d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##filename declarations\n",
    "\n",
    "base_url = \"https://case.law/download/bulk_exports/latest/by_jurisdiction/case_text_open/\"\n",
    "base_folder = \"../data\"\n",
    "folder_names = [\"ark\", \"ill\", \"nc\", \"nm\"]\n",
    "for item in folder_names:\n",
    "    os.makedirs(f\"{base_folder}/{item}\", exist_ok=True)\n",
    "\n",
    "def get_url_loc(file_name, base_url=base_url, base_folder=base_folder):\n",
    "    url = os.path.join(base_url, file_name)\n",
    "    file_loc = os.path.join(base_folder, file_name)\n",
    "    return url, file_loc    \n",
    "    \n",
    "    \n",
    "text_file_names = [f\"{item}/{item}_text.zip\" for item in folder_names]\n",
    "xml_file_names = [f\"{item}/{item}_xml.zip\" for item in folder_names]\n",
    "\n",
    "text_file_url_locs = list(map(get_url_loc, text_file_names))\n",
    "xml_file_url_locs = list(map(get_url_loc, xml_file_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9609ce5",
   "metadata": {},
   "source": [
    "** downloads **\n",
    "\n",
    "The following code downloads the raw dataset. \n",
    "\n",
    "NB: This is a onetime activity.\n",
    "\n",
    "\n",
    "```{bash}\n",
    "# download  citation graph and metadata\n",
    "!wget -c https://case.law/download/citation_graph/2021-04-20/citations.csv.gz -P ../data/\n",
    "!wget -c https://case.law/download/citation_graph/2021-04-20/metadata.csv.gz -P ../data/\n",
    "```\n",
    "\n",
    "\n",
    "```{python}\n",
    "for url, file_name in tqdm.tqdm_notebook(text_file_url_locs, total=len(text_file_names)):\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, \"wb\") as outfile:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                outfile.write(chunk)\n",
    "                \n",
    "for url, file_name in tqdm.tqdm_notebook(xml_file_url_locs, total=len(xml_file_names)):\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, \"wb\") as outfile:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                outfile.write(chunk)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb04e93e",
   "metadata": {},
   "source": [
    "## Data Loading and Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "611fa5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cases(fnames, field=\"id\"):\n",
    "    cases = []\n",
    "    for fname in fnames:\n",
    "        with zipfile.ZipFile(fname, 'r') as zip_archive:\n",
    "            xz_path = next(path for path in zip_archive.namelist() if path.endswith('/data.jsonl.xz'))\n",
    "            with zip_archive.open(xz_path) as xz_archive, lzma.open(xz_archive) as jsonlines:\n",
    "                for i, line in tqdm.tqdm_notebook(enumerate(jsonlines)):\n",
    "                    record = json.loads(str(line, 'utf-8'))\n",
    "                    if field:\n",
    "                        record = {field: record[field]}\n",
    "                    cases.append(record)\n",
    "        print(f\"loaded {i+1} cases from {fname.split('/')[-1]}\")\n",
    "    return pd.DataFrame(cases)\n",
    "\n",
    "def read_citation_graph(fname, case_ids):\n",
    "    csvobj = csv.reader(gzip.open(fname, mode='rt'),delimiter = ',',quotechar=\"'\")\n",
    "    graph = []\n",
    "    for item in tqdm.tqdm_notebook(csvobj):\n",
    "        head = item[0]\n",
    "        try:\n",
    "            head = int(head)\n",
    "        except ValueError:\n",
    "            head = None\n",
    "        if head in case_ids:\n",
    "            graph.append(item)\n",
    "    citation_graph = pd.DataFrame(graph)\n",
    "    citation_graph = citation_graph.set_index(0)\n",
    "    citation_graph = citation_graph.apply(lambda x: x.dropna().tolist(), axis=1)\n",
    "    return citation_graph\n",
    "\n",
    "\n",
    "def read_citiation_metadata(fname, case_ids):\n",
    "    csvobj = csv.DictReader(gzip.open(fname, mode='rt'),delimiter = ',')\n",
    "    graph_meta = []\n",
    "    for item in tqdm.tqdm_notebook(csvobj):\n",
    "        head = item[\"id\"]\n",
    "        try:\n",
    "            head = int(head)\n",
    "        except ValueError:\n",
    "            head = None\n",
    "        if head in case_ids:\n",
    "            graph_meta.append(item)\n",
    "    graph_meta = pd.DataFrame(graph_meta).set_index(\"id\")\n",
    "    return graph_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89f6db4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-4b5d67d4d59d>:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, line in tqdm.tqdm_notebook(enumerate(jsonlines)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1b4b182f8b4f86bd32f09275bfc120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 59735 cases from ark_text.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588dda22afb3477097e0ba29b5544aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 183033 cases from ill_text.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b90aa039d6e41bd9ad2be3a14dd4b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 97600 cases from nc_text.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84e1a198a974633ad83896a8cc4eb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 18338 cases from nm_text.zip\n"
     ]
    }
   ],
   "source": [
    "_, text_fnames= zip(*text_file_url_locs)\n",
    "cases_data = load_cases(text_fnames, field=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8547b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of cases in the raw dataset : 358706\n",
      "Total number of columns in the raw dataset : 17\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of cases in the raw dataset : {len(cases_data)}\")\n",
    "print(f\"Total number of columns in the raw dataset : {len(cases_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5682bca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": 1869772,\n",
      "  \"url\": \"https://api.capapi.org/v1/cases/1869772/\",\n",
      "  \"name\": \"James Joseph STANDLEY, Jr. v. STATE of Arkansas\",\n",
      "  \"name_abbreviation\": \"Standley v. State\",\n",
      "  \"decision_date\": \"1987-11-09\",\n",
      "  \"docket_number\": \"\",\n",
      "  \"first_page\": \"517\",\n",
      "  \"last_page\": \"518\",\n",
      "  \"citations\": [\n",
      "    {\n",
      "      \"cite\": \"293 Ark. 517\",\n",
      "      \"type\": \"official\"\n",
      "    },\n",
      "    {\n",
      "      \"cite\": \"738 S.W.2d 423\",\n",
      "      \"type\": \"parallel\"\n",
      "    }\n",
      "  ],\n",
      "  \"volume\": {\n",
      "    \"volume_number\": \"293\",\n",
      "    \"barcode\": \"32044078577145\",\n",
      "    \"url\": \"https://api.capapi.org/v1/volumes/32044078577145/\"\n",
      "  },\n",
      "  \"reporter\": {\n",
      "    \"id\": 368,\n",
      "    \"full_name\": \"Arkansas Reports\",\n",
      "    \"url\": \"https://api.capapi.org/v1/reporters/368/\"\n",
      "  },\n",
      "  \"court\": {\n",
      "    \"name_abbreviation\": \"Ark.\",\n",
      "    \"name\": \"Arkansas Supreme Court\",\n",
      "    \"id\": 8808,\n",
      "    \"slug\": \"ark\",\n",
      "    \"url\": \"https://api.capapi.org/v1/courts/ark/\"\n",
      "  },\n",
      "  \"jurisdiction\": {\n",
      "    \"name\": \"Ark.\",\n",
      "    \"id\": 34,\n",
      "    \"slug\": \"ark\",\n",
      "    \"name_long\": \"Arkansas\",\n",
      "    \"url\": \"https://api.capapi.org/v1/jurisdictions/ark/\",\n",
      "    \"whitelisted\": true\n",
      "  },\n",
      "  \"cites_to\": [\n",
      "    {\n",
      "      \"cite\": \"738 S.W.2d 423\"\n",
      "    }\n",
      "  ],\n",
      "  \"frontend_url\": \"https://cite.capapi.org/ark/293/517/\",\n",
      "  \"preview\": [],\n",
      "  \"casebody\": {\n",
      "    \"status\": \"ok\",\n",
      "    \"data\": {\n",
      "      \"judges\": [],\n",
      "      \"head_matter\": \"James Joseph STANDLEY, Jr. v. STATE of Arkansas\\n738 S.W.2d 423\\nSupreme Court of Arkansas\\nOpinion delivered November 9, 1987\\nBilly J. Allred, for appellant.\\nNo response.\",\n",
      "      \"corrections\": \"\",\n",
      "      \"opinions\": [\n",
      "        {\n",
      "          \"text\": \"Per Curiam.\\nAppellant, James Joseph Standley, Jr., by his attorney, has filed for a rule on the clerk. His attorney, Billy J. Allred, admits that the record was tendered late due to a mistake on his part.\\nWe find that such error, admittedly made by the attorney for a criminal defendant, is good cause to grant the motion. See our Per Curiam opinion dated February 5, 1979, In Re: Belated Appeals in Criminal Cases.\\nA copy of this opinion will be forwarded to the Committee on Professional Conduct.\",\n",
      "          \"author\": \"Per Curiam.\",\n",
      "          \"type\": \"majority\"\n",
      "        }\n",
      "      ],\n",
      "      \"attorneys\": [\n",
      "        \"Billy J. Allred, for appellant.\",\n",
      "        \"No response.\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "##view the first case\n",
    "sample_case = cases_data.iloc[0,:].to_json()\n",
    "#fix for escape chars\n",
    "sample_case = json.dumps(json.loads(sample_case), indent=2)\n",
    "print(sample_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31becccf",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "Each caselaw record is represented by a unique id and contains metadata relating to `decision_date`, `volume`, `reporter`, `jurisdiction`, `court` in addition to the case text that comprises of the `head_matter` and `opinions`. We proceed with extracting the and data removing empty values and columns from the dataset before diving into the text and graph processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af24820a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    True\n",
       "url                   True\n",
       "name                  True\n",
       "name_abbreviation     True\n",
       "decision_date         True\n",
       "docket_number        False\n",
       "first_page            True\n",
       "last_page             True\n",
       "citations             True\n",
       "volume                True\n",
       "reporter              True\n",
       "court                 True\n",
       "jurisdiction          True\n",
       "cites_to             False\n",
       "frontend_url          True\n",
       "preview              False\n",
       "casebody              True\n",
       "dtype: bool"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get columns with any empty values\n",
    "display(cases_data.applymap(lambda x: True if x else False).all(axis=0))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4a5daa4",
   "metadata": {},
   "source": [
    "# initally looks like `preview` and `docket_number` are sparse and don't really matter for the problem.\n",
    "# we'll get the citation information after linking the citation graph\n",
    "drop_cols = [\"docket_number\", \"preview\",\"cites_to\", \"citations\"]\n",
    "cases_data = cases_data.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0b16f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the id to int for indexing\n",
    "cases_data[\"id\"] = cases_data[\"id\"].astype(int)\n",
    "cases_data = cases_data.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ac778ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = pd.read_csv(\"../data/subset/cases.csv\", index_col=\"id\").index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "984191ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_data = cases_data.loc[cases, [\"cites_to\"]]\n",
    "cases_data.reset_index().to_json(\"../data/subset/case_citations.json\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb604c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date column\n",
    "display(cases_data[cases_data.decision_date == \"1914-02-29\"])\n",
    "cases_data[\"decision_date\"] = pd.to_datetime(cases_data[\"decision_date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "cases_data = cases_data[cases_data[\"decision_date\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b86fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.distplot(pd.DatetimeIndex(cases_data.decision_date).year, kde=False)\n",
    "plt.title(\"Distibution of cases by Year\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087aa78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract and normalize volumes\n",
    "volumes_data = pd.DataFrame(cases_data.volume.tolist(), index=cases_data.index)\n",
    "volumes_data.loc[:, \"volume_number\"] = volumes_data[\"volume_number\"].astype(int)\n",
    "cases_data.loc[:, \"volume_id\"] = volumes_data[\"volume_number\"]\n",
    "volumes_data = volumes_data.drop_duplicates(\"volume_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9730a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# volumes_data.to_csv(\"data/subset/volumes.csv\", index=False, index_label=False, quoting=csv.QUOTE_ALL, quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d26001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and normalize reporters\n",
    "reporters_data = pd.DataFrame(cases_data[\"reporter\"].tolist(), index=cases_data.index)\n",
    "reporters_data.loc[:, \"id\"] = reporters_data[\"id\"].astype(int)\n",
    "cases_data.loc[:, \"reporter_id\"] = reporters_data[\"id\"]\n",
    "reporters_data = reporters_data.drop_duplicates(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c2cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_by_reporters = cases_data.reporter_id.value_counts()\n",
    "reporter_names = reporters_data.set_index(\"id\").loc[cases_by_reporters.index, \"full_name\"].tolist()\n",
    "cases_by_reporters = cases_by_reporters.reset_index()\n",
    "cases_by_reporters['reporters']  = reporter_names\n",
    "cases_by_reporters = cases_by_reporters.rename({\"reporter_id\": \"count\", \"index\": \"reporter_id\"}, axis=1)\n",
    "cases_by_reporters[\"reporter_id\"] = cases_by_reporters[\"reporter_id\"].map(str)\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(data = cases_by_reporters,x=\"reporter_id\", y=\"count\", hue=\"reporters\", dodge=False)\n",
    "plt.xticks(None, rotation=45)\n",
    "plt.suptitle(\"Count of cases per reporters in the dataset\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781c8f5",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "North Carolina has fewest cases when compared to the other jurisdictions in the corpus.\n",
    "Most of these are however reported by the same reporter.\n",
    "Hence we observe that `North Carolina Reports` has the most number of cases associated with it in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c9b2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reporters_data.to_csv(\"data/subset/reporters.csv\", index=False, index_label=False, quoting=csv.QUOTE_ALL, quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc642918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and normalize courts\n",
    "courts_data = pd.DataFrame(cases_data[\"court\"].tolist(), index=cases_data.index)\n",
    "courts_data.loc[:, \"id\"] = courts_data[\"id\"].astype(int)\n",
    "cases_data.loc[:, \"court_id\"] = courts_data[\"id\"]\n",
    "courts_data = courts_data.drop_duplicates(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e67f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_by_courts = cases_data.court_id.value_counts()\n",
    "court_names = courts_data.set_index(\"id\").loc[cases_by_courts.index, \"name\"].tolist()\n",
    "\n",
    "cases_by_courts = cases_by_courts.reset_index()\n",
    "cases_by_courts['court']  = court_names\n",
    "cases_by_courts = cases_by_courts.rename({\"court_id\": \"count\", \"index\": \"court_id\"}, axis=1)\n",
    "cases_by_courts[\"court_id\"] = cases_by_courts[\"court_id\"].map(str)\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(data = cases_by_courts,x=\"court_id\", y=\"count\", hue=\"court\", dodge=False)\n",
    "\n",
    "\n",
    "plt.suptitle(\"Count of cases per court in the dataset\")\n",
    "plt.xticks(rotation=45)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# courts_data.to_csv(\"data/subset/courts.csv\", index=False, index_label=False, quoting=csv.QUOTE_ALL, quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25572eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and normalize jurisdictions\n",
    "jurisdictions_data = pd.DataFrame(cases_data[\"jurisdiction\"].tolist(), index=cases_data.index)\n",
    "jurisdictions_data.loc[:,\"id\"] = jurisdictions_data[\"id\"].astype(int)\n",
    "cases_data.loc[:, \"jurisdiction_id\"] = jurisdictions_data[\"id\"]\n",
    "jurisdictions_data = jurisdictions_data.drop_duplicates(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1fc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_by_jurisdictions = cases_data.jurisdiction_id.value_counts()\n",
    "jurisdiction_names = jurisdictions_data.set_index(\"id\").loc[cases_by_jurisdictions.index, \"name\"].tolist()\n",
    "\n",
    "cases_by_jurisdictions = cases_by_jurisdictions.reset_index()\n",
    "cases_by_jurisdictions['jurisdiction']  = jurisdiction_names\n",
    "cases_by_jurisdictions = cases_by_jurisdictions.rename({\"jurisdiction_id\": \"count\", \"index\": \"jurisdiction_id\"}, axis=1)\n",
    "cases_by_jurisdictions[\"jurisdiction_id\"] = cases_by_jurisdictions[\"jurisdiction_id\"].map(str)\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(data = cases_by_jurisdictions,x=\"jurisdiction_id\", y=\"count\", hue=\"jurisdiction\", dodge=False)\n",
    "\n",
    "\n",
    "plt.suptitle(\"Count of cases per jurisdiction in the dataset\")\n",
    "plt.xticks(rotation=45)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b680a3",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "\n",
    "- The dataset is skewed with Illinois having more that 175,000 cases while New Mexico has fewer that 25000 cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c56334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jurisdictions_data.to_csv(\"data/subset/jurisdictions.csv\", index=False, index_label=False, quoting=csv.QUOTE_ALL, quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437593f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract case opinion and headmatter\n",
    "casebody_data = pd.DataFrame(cases_data.loc[:, \"casebody\"].map(lambda x: x.get(\"data\")).tolist(), index=cases_data.index)\n",
    "cases_data.loc[: , \"head_matter\"] = casebody_data.loc[:, \"head_matter\"]\n",
    "cases_data.loc[: , \"opinion_text\"] = casebody_data.loc[:,\"opinions\"].map(lambda x: \"\\n\".join(y.get(\"text\", \"\") for y in x))\n",
    "cases_data[\"head_matter\"] = cases_data[\"head_matter\"]\n",
    "cases_data[\"opinion_text\"] = cases_data[\"opinion_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "sns.distplot(cases_data.head_matter.map(len), bins=100, kde=False)\n",
    "plt.suptitle(\"Distibution of character lengths of case head matter\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f14850",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(cases_data.head_matter.map(len).describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78ecfd4",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "- Long tail distribution with many texts falling under ~2000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a76df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "sns.distplot(cases_data.opinion_text.map(len), bins=100, kde=False)\n",
    "plt.suptitle(\"Distibution of character lengths of case opinion text\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d5e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(cases_data.opinion_text.map(len).describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa107ec",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "- Long tail distribution with 75% if texts falling under ~13578 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edd5596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read citation graph and link nodes\n",
    "citation_graph = read_citation_graph(\"../data/citations.csv.gz\", case_ids=frozenset(cases_data.index))\n",
    "print(f\"found and loaded {len(citation_graph)} nodes into citation_graph\")\n",
    "\n",
    "#create a lookup for our cases\n",
    "citations_uids = frozenset(citation_graph.index)\n",
    "\n",
    "# remove citations that aren't in case data\n",
    "citation_graph = citation_graph.loc[:].map(lambda x: list(filter(lambda y: y in citations_uids, x)))\n",
    "\n",
    "# remove cases with no citations after truncation\n",
    "citation_graph = citation_graph[citation_graph.map(len) > 0]\n",
    "citation_graph.index = citation_graph.index.astype(int)\n",
    "\n",
    "cases_data.loc[citation_graph.index, \"citation_ids\"] = citation_graph.values\n",
    "cases_data = cases_data[cases_data.citation_ids.notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e5ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_case_cols = [\n",
    "    'decision_date', 'name', 'name_abbreviation',\n",
    "    'frontend_url', 'url', \n",
    "    #'head_matter', 'opinion_text', \n",
    "    'volume_id', 'reporter_id', 'court_id', 'jurisdiction_id',]\n",
    "#     'citation_ids']\n",
    "# cases_data[required_case_cols].reset_index().to_csv(\"data/subset/cases.csv\", index=False, index_label=False, quoting=csv.QUOTE_ALL, quotechar='\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e087a9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations_data = cases_data[\"citation_ids\"].explode().reset_index()\n",
    "citations_data.columns = [\"src\", \"dst\"]\n",
    "# citations_data.to_csv(\"data/subset/citations.csv\", index=False, index_label=False,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2208432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# citations_data = pd.read_csv(\"../data/subset/citations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a54c1",
   "metadata": {},
   "source": [
    "Which cases cite the most number of cases in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82110a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "sns.distplot(cases_data.citation_ids.map(len), kde=False)\n",
    "plt.suptitle(\"Distibution of citations from each case in the datset\")\n",
    "plt.xlabel(\"Number of citations\")\n",
    "plt.ylabel(\"Number of cases referring to citations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8734e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(cases_data.citation_ids.map(len).describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36697c0f",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "We observe that about 75% of the data has 10 or less citations. We also observe some cases citing more than a 100 citations. This needs further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f8797",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cited = citations_data.src.value_counts().head(100)\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.barplot(top_cited.index, top_cited.values, order=top_cited.index,)\n",
    "ax.set(xlabel=\"Case ID\", ylabel = \"Count\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.suptitle(\"Top 100 cases containing the most citations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cec973",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "Some cases cite more than a hundred cases. This might be an anomaly. However, it's equally likely that it is also possible and many cases are referred to in a single caselaw. This naturally supports the idea that caselaws build use citations to build strong arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cbf94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of cases in the processed dataset : {len(cases_data)}\")\n",
    "print(f\"Total number of columns in the processed dataset : {len(cases_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51490684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# casebody_data.loc[cases_data.index].reset_index().to_json(\"../data/subset/casebody.json\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc37941",
   "metadata": {},
   "source": [
    "## Graph Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90496e0",
   "metadata": {},
   "source": [
    "We load the graph into [neo4j](https://neo4j.com/) graph database for further analysis. It can be run by running docker and the following command from the project root directory. \n",
    "\n",
    "Copy the subset csv files from the `data/subset` folder to the import folder under `neo4j/import/`\n",
    "Run the following commands.\n",
    "\n",
    "\n",
    "```{bash}\n",
    "docker run \\\n",
    "    --name caselaw-neo4j \\\n",
    "    -p7474:7474 -p7687:7687 \\\n",
    "    -d \\\n",
    "    -v $PWD/neo4j/data:/data \\\n",
    "    -v $PWD/neo4j/logs:/logs \\\n",
    "    -v $PWD/neo4j/import:/var/lib/neo4j/import \\\n",
    "    -v $PWD/neo4j/plugins:/plugins \\\n",
    "    -e NEO4J_apoc_export_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_use__neo4j__config=true \\\n",
    "    --env NEO4J_AUTH='neo4j/zaq!0pl' \\\n",
    "    --env NEO4JLABS_PLUGINS='[\"apoc\", \"graph-data-science\"]' \\\n",
    "    neo4j:latest\n",
    "```{bash}                            \n",
    "\n",
    "```{cypher}                            \n",
    "CREATE CONSTRAINT jurisdictionId IF NOT EXISTS on (jur:Jurisdiction) ASSERT jur.id IS UNIQUE;\n",
    "LOAD CSV WITH HEADERS FROM 'file:///jurisdictions.csv' AS row\n",
    "WITH row\n",
    "MERGE (jurisdiction:Jurisdiction {id:toInteger(row.id)})\n",
    "ON CREATE SET jurisdiction.name = row.name_long,\n",
    "jurisdiction.url = row.url;\n",
    "\n",
    "CREATE CONSTRAINT courtId IF NOT EXISTS on (cou:Court) ASSERT cou.id IS UNIQUE;\n",
    "LOAD CSV WITH HEADERS FROM 'file:///courts.csv' AS row\n",
    "WITH row\n",
    "MERGE (court:Court {id: toInteger(row.id)})\n",
    "ON CREATE SET court.name = row.name,\n",
    "court.url = row.url;\n",
    "\n",
    "CREATE CONSTRAINT reporterId IF NOT EXISTS on (rep:Reporter) ASSERT rep.id IS UNIQUE;\n",
    "LOAD CSV WITH HEADERS FROM 'file:///reporters.csv' AS row\n",
    "WITH row\n",
    "MERGE (reporter:Reporter {id: toInteger(row.id)})\n",
    "ON CREATE SET reporter.name = row.full_name,\n",
    "reporter.url = row.url;\n",
    "\n",
    "CREATE CONSTRAINT volumeId IF NOT EXISTS on (vol:Volume) ASSERT vol.id IS UNIQUE;\n",
    "LOAD CSV WITH HEADERS FROM 'file:///volumes.csv' AS row\n",
    "WITH row\n",
    "MERGE (volume:Volume {id: toInteger(row.volume_number)})\n",
    "ON CREATE SET volume.barcode = row.barcode,\n",
    "volume.url = volume.url;\n",
    "\n",
    "CREATE CONSTRAINT caselawId IF NOT EXISTS on (cas:Caselaw) ASSERT cas.id IS UNIQUE;\n",
    ":auto USING PERIODIC COMMIT 1000\n",
    "LOAD CSV WITH HEADERS FROM 'file:///cases.csv' AS row\n",
    "WITH row\n",
    "MATCH (volume:Volume {id: toInteger(row.volume_id)})\n",
    "MATCH (court:Court {id: toInteger(row.court_id)})\n",
    "MATCH (reporter:Reporter {id: toInteger(row.reporter_id)})\n",
    "MATCH (jurisdiction:Jurisdiction {id: toInteger(row.jurisdiction_id)})\n",
    "MERGE (caselaw:Caselaw {id: toInteger(row.id)})\n",
    "MERGE (caselaw) -[:REPORTED_BY]->(reporter)\n",
    "MERGE (caselaw) -[:HEARD_BY]->(court)\n",
    "MERGE (caselaw) -[:IN_VOLUME]->(volume)\n",
    "MERGE (caselaw) -[:UNDER_JURISIDICTION]->(jurisdiction)\n",
    "on CREATE SET caselaw.decision_date = datetime(row.decision_date),\n",
    "caselaw.name = row.name,\n",
    "caselaw.url = row.url;\n",
    "\n",
    ":auto USING PERIODIC COMMIT 1000\n",
    "LOAD CSV WITH HEADERS FROM 'file:///citations.csv' AS row\n",
    "with row\n",
    "MATCH (case1: Caselaw {id: toInteger(row.src)})\n",
    "MATCH (case2: Caselaw {id: toInteger(row.dst)})\n",
    "MERGE (case1) -[:CITED]-> (case2);\n",
    "\n",
    "```\n",
    "\n",
    "For pre installed database copy the `neo4j` folder from the following drive link https://drive.google.com/drive/folders/1afRp7eMZqIyScYoC1ELdnrKMwfE4YUJh?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a819073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample case graph visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f0a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"A sample representation of a caselaw in the graph database\")\n",
    "Image(filename='../reports/Sample Case law network.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(password=\"zaq!0pl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4727c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most cited cases in a jurisdiction ?\n",
    "display(graph.run(\"\"\"\n",
    "MATCH g=(j1:Jurisdiction)<-[:UNDER_JURISIDICTION]-(c1:Caselaw)<-[:CITED]-(c2)\n",
    "with j1, c1, count(DISTINCT c2) as citations\n",
    "ORDER BY citations desc\n",
    "WITH j1.name as Jurisdiction, collect(c1.id)[0] as case_id,\n",
    "collect(c1.name)[0] as case_name, collect(citations)[0] as num_citations\n",
    "RETURN Jurisdiction, case_id, case_name, num_citations\n",
    "ORDER BY Jurisdiction ASC\n",
    "\"\"\").to_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91034af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(graph.run(\"\"\"MATCH g=(j1:Jurisdiction)<-[:UNDER_JURISIDICTION]-(c1:Caselaw)<-[:CITED]-(c2)\n",
    "with j1, c1, count(DISTINCT c2) as citations\n",
    "ORDER BY citations desc\n",
    "WITH j1.name as Jurisdiction, collect({caselaw: c1.name, citations: citations}) as top_k\n",
    "RETURN Jurisdiction, top_k[0..5]\n",
    "ORDER BY Jurisdiction DESC\n",
    "\"\"\").to_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088feec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(graph.run(\"\"\"MATCH (j1:Jurisdiction)<-[:UNDER_JURISIDICTION]-(c1:Caselaw)-[:CITED]-(c2:Caselaw)-[:UNDER_JURISIDICTION]->(j2:Jurisdiction)\n",
    "WHERE j1 <> j2\n",
    "WITH j1, j2, count(DISTINCT c2) as cids\n",
    "ORDER BY cids DESC\n",
    "RETURN j1.name as src_Jurisdiction, collect(j2.name)[0] as dest_Jurisdiction, cids as cited_cases;\"\"\").to_table())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027e02d",
   "metadata": {},
   "source": [
    "*How are cases from one jurisdiction related to another ?*\n",
    "\n",
    "The following query shows a sample of cases that connect different jurisdictions in the graph.\n",
    "\n",
    "```{cypher}\n",
    "MATCH (co1:Court)<-[:HEARD_BY]-(c1:Caselaw)-[:CITED]-(c2:Caselaw)-[:HEARD_BY]->(co2:Court),\n",
    "(c1)-[:UNDER_JURISIDICTION]->(j1:Jurisdiction)\n",
    "WHERE not((c2)-[:UNDER_JURISIDICTION]->(j1))\n",
    "RETURN co1, co2, c1, c2, j1\n",
    "LIMIT 20;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125e0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../reports/Cases relating one jurisdiction to another.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dcd1d6",
   "metadata": {},
   "source": [
    "How about cases citing from courts in other jurisdictions ?\n",
    "\n",
    "The following query allows us to retrive a sample of cases that linked to courts in Arkansas and courts from other Jurisdictions.\n",
    "\n",
    "```\n",
    "MATCH (j1:Jurisdiction)<-[:UNDER_JURISIDICTION]-(c1:Caselaw)-[:CITED]-(c2:Caselaw)-[:UNDER_JURISIDICTION]->(j2:Jurisdiction)\n",
    "WHERE j1 <> j2\n",
    "RETURN j2,c1, c2, j1\n",
    "LIMIT 20;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2256f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../reports/sample_jurisdiction court_graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed82f55",
   "metadata": {},
   "source": [
    "## Text Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45eaf79a",
   "metadata": {},
   "source": [
    "**Preprocessing**\n",
    "\n",
    "As we saw the case texts are quite large approx. (28k-50k characters). The texts also contaim misspellings and OCR errors. Additionally, we see hints from reviewing a few sample cases that there are mentions of person names, organizations and other entities such as dates in the case data that might be relevant in idenifying the facts of a case. To preprocess the text, we first concatenate the case text (`Head matter` and `Opinions`) into a single field and proceed to preprocess the text.\n",
    "\n",
    "**Preprocessing Steps**\n",
    " - We parse the text applying POS tagging to the text using [spacy](https://spacy.io/).\n",
    " - We retain only words that are tagged as `Adjectives`, `Adverbs`, `Nouns`, `Proper Nouns` and `Pronouns`.\n",
    " - This reduces the vocabulary size and reduces the dimensionality of the data by a large factor and makes it feasable to work with the text in the dataset for our Exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af050a",
   "metadata": {},
   "source": [
    "```{python}\n",
    "###\n",
    "### CAUTION TAKE REALLY LONG TIME TO RUN\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "casetext = casebody_data[\"head_matter\"] + \"\\n\"+ casebody_data[\"opinions\"].map(lambda x: \" \".join(y.get(\"text\", \"\") for y in x))\n",
    "with open(\"../data/subset/caselines.txt\", \"w+\") as outfile:\n",
    "    tags = frozenset([\"ADJ\",\"ADV\",\"NOUN\",\"PRON\",\"PROPN\"])\n",
    "    for doc in tqdm.tqdm_notebook(nlp.pipe(casetext,batch_size=10, n_process=-1), total=len(casetext)):\n",
    "        tokens = []\n",
    "        for tok in doc:\n",
    "            if tok.pos_ in tags and tok.is_alpha:\n",
    "                tokens.append(tok.lemma_.lower())\n",
    "        tokens = \" \".join(tokens)\n",
    "        tokens = preprocess_string(tokens,DEFAULT_FILTERS[:-1])\n",
    "        outline = \" \".join(tokens) + \"\\n\"\n",
    "        outfile.write(outline)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e34e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_lines = pd.read_csv(\"../data/subset/caselines.txt\", header=None, names=[\"text\"])\n",
    "case_lines.index=cases_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df244c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases_data = cases_data[[\"head_matter\", \"jurisdiction_id\", \"court_id\", \"name_abbreviation\", \"citation_ids\"]]\n",
    "cases_data[\"citation_ids\"] = cases_data.citation_ids.map(lambda x: [int(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845ce608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import option_context\n",
    "# sample preprocessed text\n",
    "with option_context('display.max_colwidth', 400):\n",
    "    display(case_lines.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c5e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst):\n",
    "    return (item for sublist in lst for item in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da0d3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_vocab = {}\n",
    "groups = cases_data.groupby(\"jurisdiction_id\").groups.items()\n",
    "for grp,idx in groups:\n",
    "    grp_data = case_lines.loc[idx]\n",
    "    top_10 = Counter(flatten(grp_data.text.str.split().tolist())).most_common(10)\n",
    "    grp_vocab[grp] = top_10\n",
    "num_plots = len(grp_vocab)\n",
    "fig, ax = plt.subplots(nrows=num_plots, ncols=1, figsize=(20, 20))\n",
    "for i, (k,v) in enumerate(grp_vocab.items()):\n",
    "    title = jurisdictions_data[jurisdictions_data.id == k]['name_long'].values[0]\n",
    "    word, count = zip(*v)\n",
    "    sns.barplot(x=np.array(word),y=np.array(count),ax=ax[i])\n",
    "    ax[i].set_title(f\"Jurisdiction:{title}\")\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.suptitle(\"Top 10 words by Jurisdiction\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fda1c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grp_vocab = {}\n",
    "groups = cases_data.groupby(\"court_id\").groups.items()\n",
    "for grp,idx in groups:\n",
    "    grp_data = case_lines.loc[idx]\n",
    "    top_10 = Counter(flatten(grp_data.text.str.split().tolist())).most_common(10)\n",
    "    grp_vocab[grp] = top_10\n",
    "num_plots = len(grp_vocab)\n",
    "fig, ax = plt.subplots(nrows=num_plots, ncols=1, figsize=(20, 40))\n",
    "for i, (k,v) in enumerate(grp_vocab.items()):\n",
    "    title = courts_data[courts_data.id == k]['name'].values[0]\n",
    "    word, count = zip(*v)\n",
    "    sns.barplot(x=np.array(word),y=np.array(count),ax=ax[i])\n",
    "    ax[i].set_title(f\"Court:{title}\")\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.suptitle(\"Top 10 words by Court\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29330c6c",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "- Most of the top words are the same among the Jurisdictions and include: \n",
    "- We observe a small variation among the top words in the Courts For instance `court of appeals`  and `appallette courts` frequently feature the terms such as `defendent` and `appellent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041a9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_data[\"year\"] = cases_data['decision_date'].dt.year\n",
    "\n",
    "year_vocab = {}\n",
    "year = [1813,1830,1870,1910,1947,1950,1990, 2000,2010, 2018]\n",
    "groups = cases_data.groupby(\"year\").groups.items()\n",
    "for grp,idx in groups :\n",
    "    if grp in year:\n",
    "        grp_data = case_lines.loc[idx]\n",
    "        top_5 = Counter(flatten(grp_data.text.str.split().tolist())).most_common(5)\n",
    "        year_vocab[grp] = top_5\n",
    "    \n",
    "num_plots = len(year_vocab)\n",
    "fig, ax = plt.subplots(nrows=num_plots, ncols=1, figsize=(20, 30))\n",
    "for i, (k,v) in enumerate(year_vocab.items()):\n",
    "    title = k\n",
    "    word, count = zip(*v)\n",
    "    sns.barplot(x=np.array(word),y=np.array(count),ax=ax[i])\n",
    "    ax[i].set_title(f\"\\n\\nYear:{title}\" , fontsize=20)\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.subplots_adjust(hspace = 0.8)\n",
    "plt.suptitle(\"Top 5 words by Year\", fontsize=30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9639c92",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c4d45f",
   "metadata": {},
   "source": [
    "**A Simple Baseline: Text Similarity for recommending citations.**\n",
    "\n",
    "We hypothesize that cited caselaws often share similar vocabulary with the citing case text. This motivate us to explore a similarity metrics such since cosine similarity among document vectors to retrieve relevant documents given a query document.\n",
    "\n",
    "**Methodology**\n",
    "\n",
    "- We convert the preprocessed text to a bag-of-words representation. \n",
    "- The vocabulary contains ~538954 words. \n",
    "- We reduce the dimensionality of the data by pruning the vocabulary to retain the 50000 most common words. \n",
    "- Next we create TFIDF vectors of the documents and measure the cosine similarity of the tfidf vectors.\n",
    "- We then compute the mean cosine similarity of a sample of documents in the corpus with their citations to answer the question- `How similar are cited cases to each other ?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67265a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CaselineIterator:\n",
    "#     def __init__(self, fname, slice_size=None):\n",
    "#         self.fname = fname\n",
    "#         self.slice_size=slice_size\n",
    "#         self._length = slice_size\n",
    "#     def __iter__(self):\n",
    "#         if self.slice_size:\n",
    "#             iterator = islice(open(self.fname), self.slice_size)\n",
    "#         else:\n",
    "#             iterator = open(self.fname)\n",
    "#         for line in iterator:\n",
    "#             yield line.split()\n",
    "            \n",
    "#     def __len__(self):\n",
    "#         if not self._length:\n",
    "#             self._length = sum(1 for _ in open(self.fname))\n",
    "#         return self._length\n",
    "\n",
    "class CaselawCorpus():\n",
    "    def __init__(self, iterator, dictionary, **kwargs):\n",
    "        self.iterator = iterator\n",
    "        self.dictionary = dictionary\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for line in self.iterator:\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield self.dictionary.doc2bow(line)\n",
    "    def __len__(self):\n",
    "        return len(self.iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d20c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSimilariyIndexer:\n",
    "    def __init__(self, no_below=5, no_above=0.75):\n",
    "        self.dictionary = None\n",
    "        self.corpus = None\n",
    "        self.model = None\n",
    "        self.index = None\n",
    "        self.no_below=no_below\n",
    "        self.no_above=no_above\n",
    "    \n",
    "    def fit(self, texts, **kwargs):\n",
    "        self.dictionary = Dictionary(texts)\n",
    "        self.dictionary.filter_extremes(self.no_below, self.no_above, keep_n=50000)\n",
    "        self.dictionary.compactify()\n",
    "        \n",
    "        self.corpus = CaselawCorpus(texts, self.dictionary,**kwargs)\n",
    "        self.model = TfidfModel(self.corpus)\n",
    "        self.index = SparseMatrixSimilarity(\n",
    "            self.model[self.corpus], \n",
    "            num_features=len(self.dictionary),\n",
    "            num_terms=len(self.dictionary),\n",
    "            num_docs=len(self.corpus),\n",
    "            maintain_sparsity=True)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, texts, top_k=5):\n",
    "        pred_corpus = CaselawCorpus(texts, self.dictionary)\n",
    "        pred_vecs = self.model[pred_corpus]\n",
    "        pred_sims = self.index[pred_vecs]\n",
    "        return pred_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2eae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_indexer = CosineSimilariyIndexer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88ba81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del cases_data, case_lines, casebody_data, citation_graph, citations_data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8145f78",
   "metadata": {},
   "source": [
    "case_info = cases_data[[\"jurisdiction_id\", \"court_id\", \"decision_date\", \"head_matter\", \"opinion_text\", \"citation_ids\"]] "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1fc469dc",
   "metadata": {},
   "source": [
    "case_info = case_info.reset_index()\n",
    "case_info.to_json(\"../data/subset/case_info.json\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867775e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_data = pd.read_json(\"../data/subset/case_info.json\", lines=True, orient=\"records\")\n",
    "case_lines = pd.read_csv(\"../data/subset/caselines.txt\", header=None, names=[\"text\"])\n",
    "case_lines.index=cases_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd13ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2cases = pd.Series(cases_data.index, index=pd.Index(cases_data[\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccea965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a lookup for our cases\n",
    "citations_uids = frozenset(cases_data[\"id\"])\n",
    "\n",
    "# remove citations that aren't in case data\n",
    "cases_data[\"citation_ids\"] = cases_data.citation_ids.map(lambda x: list(filter(lambda y: y in citations_uids, x)))\n",
    "cases_data[\"citation_idxs\"] = cases_data.citation_ids.map(lambda x: id2cases[x].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f613a386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similarity_indexer.fit(case_lines.text.str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e1a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we sample a few cases to validate our hypothesis related to text similarity\n",
    "# note this is still training set and not a dev or test set\n",
    "sample_case_lines = case_lines.sample(1000)\n",
    "with pd.option_context('display.max_colwidth', 400):\n",
    "    display(sample_case_lines.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb99a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get similairty of queries to corpus\n",
    "sample_preds = similarity_indexer.predict(sample_case_lines.text.str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions into dense matrix\n",
    "sample_preds = sample_preds.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489bcc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean similarities of citations and their corresponding documents\n",
    "all_sims = []\n",
    "for item in tqdm.tqdm_notebook(cases_data.loc[sample_case_lines.index].citation_idxs):\n",
    "    for idx, sim in zip(item, sample_preds):\n",
    "        \n",
    "        mean_sim = sim[idx].mean()\n",
    "        all_sims.append(mean_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ddfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean cosine similarity of a sample of 1000 cases to their citations: {np.mean(all_sims):2.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb6b00",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    " - Very low mean similarities among cited documents\n",
    " - This is perhaps due to the high dimensionality of data.\n",
    " - TFIDF doesn't capture other factors such as jurisdiction, court and date in similarity calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea259181",
   "metadata": {},
   "source": [
    "**A simple heuristic to improve our baseline**\n",
    "\n",
    "Since we know that cases are more often cited among Jurisdictions and courts we use this to arrive at a baseline when used alongside our cosine similarity index. We take the following approach during inference in out baseline:\n",
    "\n",
    "While predicting the recommendations include only documents from the corpus that have:\n",
    "    - the same jurisdiction_id\n",
    "    - the same court_id and\n",
    "    - has a data before the query document date.\n",
    "    \n",
    "These assumptions are valid even for documents that are quried for citations recommendations. i.e. even for unknown documents we know what `court` and `jurisdiction` a case if being decided at when referring to a caselaw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a90a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "top_preds= []\n",
    "for (idx, row), sample_pred in tqdm.tqdm_notebook(zip(cases_data.loc[sample_case_lines.index].iterrows(), sample_preds)):\n",
    "    row = row.to_dict()\n",
    "    \n",
    "    jur_id = row[\"jurisdiction_id\"]\n",
    "    court_id = row[\"court_id\"]\n",
    "    decision_date = row[\"decision_date\"]\n",
    "    sample_pred = sample_pred.argsort()[::-1]\n",
    "    pred = sample_pred[\n",
    "        (cases_data.jurisdiction_id == jur_id) &\\\n",
    "        (cases_data.court_id == court_id) &\\\n",
    "        (cases_data.decision_date > decision_date)]\n",
    "    top_preds.append(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ddfad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_k(y_true, y_pred, k=10):\n",
    "    precisions =[]\n",
    "    recalls = []\n",
    "    for y_t, y_p in zip(y_true, y_pred):\n",
    "        y_p = y_p[:k]\n",
    "        relevant_retrieved = set(y_t).intersection(y_p)\n",
    "        if relevant_retrieved:\n",
    "            precision = len(relevant_retrieved)/len(y_p)\n",
    "            recall = len(relevant_retrieved)/len(y_t)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "    return np.mean(precisions), np.mean(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d1a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions =[]\n",
    "recalls = []\n",
    "for i in range(3,28, 3):\n",
    "    precision, recall =  precision_recall_k(cases_data.loc[sample_case_lines.index].citation_idxs.values, top_preds, i)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03948956",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_plot_df = pd.DataFrame(zip(precisions, recalls), columns=[\"precision\", \"recall\"], index=range(3,30,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49f0a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.lineplot(data=pk_plot_df, markers=True)\n",
    "plt.title(\"top-k Precision and recall for recommended samples from baseline model\")\n",
    "plt.xlabel(\"top-k\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a571cd",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "- Not much gain in recall even as we increase the number of retrieved documents.\n",
    "- Precision drops very steeply after 5 documents.\n",
    "- Top-5 results seem to have the best precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0870b1d",
   "metadata": {},
   "source": [
    "### Questions and Next steps \n",
    "\n",
    "- How do we build a train and test set when we need know they overlap in terms of cited and citing documents ?\n",
    "- It looks like a simple information retereival approch is not enough when building caselaw recommendation systems.\n",
    "- How do we represent the global and local context of a citation when making a recommendations ?\n",
    "- Can we exploit the graph structure to derive secondary references of citations to allow for deeper research in recommendations ?\n",
    "- Explore building graph based embeddings that can be used to rerank the retrieved results by only use text features as input.\n",
    "- Explore polarity of citations based on opinion `dissent`,`majority` and `concurrent` types of opinions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
