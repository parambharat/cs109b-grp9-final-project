{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature Review and Preliminary Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the conda virtual environment of this project use the following commands.\n",
    "\n",
    "```\n",
    "!conda env create -f environment.yml\n",
    "!conda activate cs109b-grp9-final-project\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import tqdm\n",
    "\n",
    "import zipfile\n",
    "import lzma\n",
    "import json\n",
    "\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO  # ipython sometimes messes up the logging setup; restore\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import SparseMatrixSimilarity\n",
    "import numpy as np\n",
    "\n",
    "from py2neo import Graph\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "\n",
    "> CAP includes all official, book-published United States case law â€” every volume designated as an official report of decisions by a court within the United States.\n",
    ">\n",
    ">Each volume has been converted into structured, case-level data broken out by majority and dissenting opinion, with >human-checked metadata for party names, docket number, citation, and date.\n",
    ">\n",
    "> -- <cite>[Case.law][1]</cite>\n",
    "\n",
    "In the scope of this analysis we use the latest open case text from from bulk data exxports of open jurisdictions in the dataset. This subset consists of caselaws and metadata for the following four Jurisdictions:\n",
    "\n",
    "1. ark - Arkansas\n",
    "2. ill - Illinois\n",
    "3. nc - North Carolina\n",
    "4. nm - New Mexico\n",
    "\n",
    "These were downloaded from [here][2].\n",
    "\n",
    "\n",
    "**Citation Graph**\n",
    "\n",
    "In addition to the case text we also use the case citation graph that links extracted, verified and unambiguous  citations to cases within the dataset. We use the citation graph from [2021-04-20][3] for this study. The citation graph is present in the form of a edgelist linking one case_id to multiple case_ids. It also contains all the references to all nodes from the case.law dataset. We truncate the graph based on case_ids we found in the case texts for each jurisdiction during preprocessing.\n",
    "\n",
    "[1]: https://case.law/about/\n",
    "[2]: https://case.law/download/bulk_exports/latest/by_jurisdiction/case_text_open/\n",
    "[3]: https://case.law/download/citation_graph/2021-04-20/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##filename declarations\n",
    "\n",
    "base_url = \"https://case.law/download/bulk_exports/latest/by_jurisdiction/case_text_open/\"\n",
    "base_folder = \"../data\"\n",
    "folder_names = [\"ark\", \"ill\", \"nc\", \"nm\"]\n",
    "for item in folder_names:\n",
    "    os.makedirs(f\"{base_folder}/{item}\", exist_ok=True)\n",
    "\n",
    "def get_url_loc(file_name, base_url=base_url, base_folder=base_folder):\n",
    "    url = os.path.join(base_url, file_name)\n",
    "    file_loc = os.path.join(base_folder, file_name)\n",
    "    return url, file_loc    \n",
    "    \n",
    "    \n",
    "text_file_names = [f\"{item}/{item}_text.zip\" for item in folder_names]\n",
    "xml_file_names = [f\"{item}/{item}_xml.zip\" for item in folder_names]\n",
    "\n",
    "text_file_url_locs = list(map(get_url_loc, text_file_names))\n",
    "xml_file_url_locs = list(map(get_url_loc, xml_file_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** downloads **\n",
    "\n",
    "The following code downloads the raw dataset. \n",
    "\n",
    "NB: This is a onetime activity.\n",
    "\n",
    "\n",
    "```{bash}\n",
    "# download  citation graph and metadata\n",
    "!wget -c https://case.law/download/citation_graph/2021-04-20/citations.csv.gz -P ../data/\n",
    "!wget -c https://case.law/download/citation_graph/2021-04-20/metadata.csv.gz -P ../data/\n",
    "```\n",
    "\n",
    "\n",
    "```{python}\n",
    "for url, file_name in tqdm.tqdm_notebook(text_file_url_locs, total=len(text_file_names)):\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, \"wb\") as outfile:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                outfile.write(chunk)\n",
    "                \n",
    "for url, file_name in tqdm.tqdm_notebook(xml_file_url_locs, total=len(xml_file_names)):\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, \"wb\") as outfile:\n",
    "            for chunk in response.iter_content(chunk_size=128):\n",
    "                outfile.write(chunk)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cases(fnames, field=\"id\"):\n",
    "    cases = []\n",
    "    for fname in fnames:\n",
    "        with zipfile.ZipFile(fname, 'r') as zip_archive:\n",
    "            xz_path = next(path for path in zip_archive.namelist() if path.endswith('/data.jsonl.xz'))\n",
    "            with zip_archive.open(xz_path) as xz_archive, lzma.open(xz_archive) as jsonlines:\n",
    "                for i, line in tqdm.tqdm_notebook(enumerate(jsonlines)):\n",
    "                    record = json.loads(str(line, 'utf-8'))\n",
    "                    if field:\n",
    "                        record = {field: record[field]}\n",
    "                    cases.append(record)\n",
    "        print(f\"loaded {i+1} cases from {fname.split('/')[-1]}\")\n",
    "    return pd.DataFrame(cases)\n",
    "\n",
    "def read_citation_graph(fname, case_ids):\n",
    "    csvobj = csv.reader(gzip.open(fname, mode='rt'),delimiter = ',',quotechar=\"'\")\n",
    "    graph = []\n",
    "    for item in tqdm.tqdm_notebook(csvobj):\n",
    "        head = item[0]\n",
    "        try:\n",
    "            head = int(head)\n",
    "        except ValueError:\n",
    "            head = None\n",
    "        if head in case_ids:\n",
    "            graph.append(item)\n",
    "    citation_graph = pd.DataFrame(graph)\n",
    "    citation_graph = citation_graph.set_index(0)\n",
    "    citation_graph = citation_graph.apply(lambda x: x.dropna().tolist(), axis=1)\n",
    "    return citation_graph\n",
    "\n",
    "\n",
    "def read_citiation_metadata(fname, case_ids):\n",
    "    csvobj = csv.DictReader(gzip.open(fname, mode='rt'),delimiter = ',')\n",
    "    graph_meta = []\n",
    "    for item in tqdm.tqdm_notebook(csvobj):\n",
    "        head = item[\"id\"]\n",
    "        try:\n",
    "            head = int(head)\n",
    "        except ValueError:\n",
    "            head = None\n",
    "        if head in case_ids:\n",
    "            graph_meta.append(item)\n",
    "    graph_meta = pd.DataFrame(graph_meta).set_index(\"id\")\n",
    "    return graph_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, text_fnames= zip(*text_file_url_locs)\n",
    "cases_data = load_cases(text_fnames, field=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of cases in the raw dataset : {len(cases_data)}\")\n",
    "print(f\"Total number of columns in the raw dataset : {len(cases_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##view the first case\n",
    "sample_case = cases_data.iloc[0,:].to_json()\n",
    "#fix for escape chars\n",
    "sample_case = json.dumps(json.loads(sample_case), indent=2)\n",
    "print(sample_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "Each caselaw record is represented by a unique id and contains metadata relating to `decision_date`, `volume`, `reporter`, `jurisdiction`, `court` in addition to the case text that comprises of the `head_matter` and `opinions`. We proceed with extracting the and data removing empty values and columns from the dataset before diving into the text and graph processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get columns with any empty values\n",
    "display(cases_data.applymap(lambda x: True if x else False).all(axis=0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# initally looks like `preview` and `docket_number` are sparse and don't really matter for the problem.\n",
    "# we'll get the citation information after linking the citation graph\n",
    "drop_cols = [\"docket_number\", \"preview\",\"cites_to\", \"citations\"]\n",
    "cases_data = cases_data.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the id to int for indexing\n",
    "cases_data[\"id\"] = cases_data[\"id\"].astype(int)\n",
    "cases_data = cases_data.set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date column\n",
    "display(cases_data[cases_data.decision_date == \"1914-02-29\"])\n",
    "cases_data[\"decision_date\"] = pd.to_datetime(cases_data[\"decision_date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "cases_data = cases_data[cases_data[\"decision_date\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.distplot(pd.DatetimeIndex(cases_data.decision_date).year, kde=False)\n",
    "plt.title(\"Distibution of cases by Year\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract and normalize volumes\n",
    "volumes_data = pd.DataFrame(cases_data.volume.tolist(), index=cases_data.index)\n",
    "volumes_data.loc[:, \"volume_number\"] = volumes_data[\"volume_number\"].astype(int)\n",
    "cases_data.loc[:, \"volume_id\"] = volumes_data[\"volume_number\"]\n",
    "volumes_data = volumes_data.drop_duplicates(\"volume_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volumes_data.to_csv(\"../data/subset/volumes.csv\", index=False, index_label=False, quoting=csv.QUOTE_ALL, quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and normalize reporters\n",
    "reporters_data = pd.DataFrame(cases_data[\"reporter\"].tolist(), index=cases_data.index)\n",
    "reporters_data.loc[:, \"id\"] = reporters_data[\"id\"].astype(int)\n",
    "cases_data.loc[:, \"reporter_id\"] = reporters_data[\"id\"]\n",
    "reporters_data = reporters_data.drop_duplicates(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_by_reporters = cases_data.reporter_id.value_counts()\n",
    "reporter_names = reporters_data.set_index(\"id\").loc[cases_by_reporters.index, \"full_name\"].tolist()\n",
    "cases_by_reporters = cases_by_reporters.reset_index()\n",
    "cases_by_reporters['reporters']  = reporter_names\n",
    "cases_by_reporters = cases_by_reporters.rename({\"reporter_id\": \"count\", \"index\": \"reporter_id\"}, axis=1)\n",
    "cases_by_reporters[\"reporter_id\"] = cases_by_reporters[\"reporter_id\"].map(str)\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(data = cases_by_reporters,x=\"reporter_id\", y=\"count\", hue=\"reporters\", dodge=False)\n",
    "plt.xticks(None, rotation=45)\n",
    "plt.suptitle(\"Count of cases per reporters in the dataset\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "North Carolina has fewest cases when compared to the other jurisdictions in the corpus.\n",
    "Most of these are however reported by the same reporter.\n",
    "Hence we observe that `North Carolina Reports` has the most number of cases associated with it in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporters_data.to_csv(\"../data/subset/reporters.csv\", index=False, index_label=False, quoting=csv.QUOTE_ALL, quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and normalize courts\n",
    "courts_data = pd.DataFrame(cases_data[\"court\"].tolist(), index=cases_data.index)\n",
    "courts_data.loc[:, \"id\"] = courts_data[\"id\"].astype(int)\n",
    "cases_data.loc[:, \"court_id\"] = courts_data[\"id\"]\n",
    "courts_data = courts_data.drop_duplicates(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_by_courts = cases_data.court_id.value_counts()\n",
    "court_names = courts_data.set_index(\"id\").loc[cases_by_courts.index, \"name\"].tolist()\n",
    "\n",
    "cases_by_courts = cases_by_courts.reset_index()\n",
    "cases_by_courts['court']  = court_names\n",
    "cases_by_courts = cases_by_courts.rename({\"court_id\": \"count\", \"index\": \"court_id\"}, axis=1)\n",
    "cases_by_courts[\"court_id\"] = cases_by_courts[\"court_id\"].map(str)\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(data = cases_by_courts,x=\"court_id\", y=\"count\", hue=\"court\", dodge=False)\n",
    "\n",
    "\n",
    "plt.suptitle(\"Count of cases per court in the dataset\")\n",
    "plt.xticks(rotation=45)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "courts_data.to_csv(\"../data/subset/courts.csv\", index=False, index_label=False, quoting=csv.QUOTE_ALL, quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and normalize jurisdictions\n",
    "jurisdictions_data = pd.DataFrame(cases_data[\"jurisdiction\"].tolist(), index=cases_data.index)\n",
    "jurisdictions_data.loc[:,\"id\"] = jurisdictions_data[\"id\"].astype(int)\n",
    "cases_data.loc[:, \"jurisdiction_id\"] = jurisdictions_data[\"id\"]\n",
    "jurisdictions_data = jurisdictions_data.drop_duplicates(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_by_jurisdictions = cases_data.jurisdiction_id.value_counts()\n",
    "jurisdiction_names = jurisdictions_data.set_index(\"id\").loc[cases_by_jurisdictions.index, \"name\"].tolist()\n",
    "\n",
    "cases_by_jurisdictions = cases_by_jurisdictions.reset_index()\n",
    "cases_by_jurisdictions['jurisdiction']  = jurisdiction_names\n",
    "cases_by_jurisdictions = cases_by_jurisdictions.rename({\"jurisdiction_id\": \"count\", \"index\": \"jurisdiction_id\"}, axis=1)\n",
    "cases_by_jurisdictions[\"jurisdiction_id\"] = cases_by_jurisdictions[\"jurisdiction_id\"].map(str)\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.barplot(data = cases_by_jurisdictions,x=\"jurisdiction_id\", y=\"count\", hue=\"jurisdiction\", dodge=False)\n",
    "\n",
    "\n",
    "plt.suptitle(\"Count of cases per jurisdiction in the dataset\")\n",
    "plt.xticks(rotation=45)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "\n",
    "- The dataset is skewed with Illinois having more that 175,000 cases while New Mexico has fewer that 25000 cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jurisdictions_data.to_csv(\"../data/subset/jurisdictions.csv\", index=False, index_label=False, quoting=csv.QUOTE_ALL, quotechar='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract case opinion and headmatter\n",
    "casebody_data = pd.DataFrame(cases_data.loc[:, \"casebody\"].map(lambda x: x.get(\"data\")).tolist(), index=cases_data.index)\n",
    "cases_data.loc[: , \"head_matter\"] = casebody_data.loc[:, \"head_matter\"]\n",
    "cases_data.loc[: , \"opinion_text\"] = casebody_data.loc[:,\"opinions\"].map(lambda x: \"\\n\".join(y.get(\"text\", \"\") for y in x))\n",
    "cases_data[\"head_matter\"] = cases_data[\"head_matter\"]\n",
    "cases_data[\"opinion_text\"] = cases_data[\"opinion_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "sns.distplot(cases_data.head_matter.map(len), bins=100, kde=False)\n",
    "plt.suptitle(\"Distibution of character lengths of case head matter\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(cases_data.head_matter.map(len).describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "- Long tail distribution with many texts falling under ~2000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "sns.distplot(cases_data.opinion_text.map(len), bins=100, kde=False)\n",
    "plt.suptitle(\"Distibution of character lengths of case opinion text\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(cases_data.opinion_text.map(len).describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "- Long tail distribution with 75% if texts falling under ~13578 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read citation graph and link nodes\n",
    "citation_graph = read_citation_graph(\"../data/citations.csv.gz\", case_ids=frozenset(cases_data.index))\n",
    "print(f\"found and loaded {len(citation_graph)} nodes into citation_graph\")\n",
    "\n",
    "#create a lookup for our cases\n",
    "citations_uids = frozenset(citation_graph.index)\n",
    "\n",
    "# remove citations that aren't in case data\n",
    "citation_graph = citation_graph.loc[:].map(lambda x: list(filter(lambda y: y in citations_uids, x)))\n",
    "\n",
    "# remove cases with no citations after truncation\n",
    "citation_graph = citation_graph[citation_graph.map(len) > 0]\n",
    "citation_graph.index = citation_graph.index.astype(int)\n",
    "\n",
    "cases_data.loc[citation_graph.index, \"citation_ids\"] = citation_graph.values\n",
    "cases_data = cases_data[cases_data.citation_ids.notnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_case_cols = [\n",
    "    'decision_date', 'name', 'name_abbreviation',\n",
    "    'frontend_url', 'url', \n",
    "    #'head_matter', 'opinion_text', \n",
    "    'volume_id', 'reporter_id', 'court_id', 'jurisdiction_id',]\n",
    "#     'citation_ids']\n",
    "# cases_data[required_case_cols].reset_index().to_csv(\"data/subset/cases.csv\", index=False, index_label=False, quoting=csv.QUOTE_ALL, quotechar='\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations_data = cases_data[\"citation_ids\"].explode().reset_index()\n",
    "citations_data.columns = [\"src\", \"dst\"]\n",
    "citations_data.to_csv(\"../data/subset/citations.csv\", index=False, index_label=False,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# citations_data = pd.read_csv(\"../data/subset/citations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which cases cite the most number of cases in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "sns.distplot(cases_data.citation_ids.map(len), kde=False)\n",
    "plt.suptitle(\"Distibution of citations from each case in the datset\")\n",
    "plt.xlabel(\"Number of citations\")\n",
    "plt.ylabel(\"Number of cases referring to citations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(cases_data.citation_ids.map(len).describe()).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "We observe that about 75% of the data has 10 or less citations. We also observe some cases citing more than a 100 citations. This needs further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cited = citations_data.src.value_counts().head(100)\n",
    "plt.figure(figsize=(20,10))\n",
    "ax = sns.barplot(top_cited.index, top_cited.values, order=top_cited.index,)\n",
    "ax.set(xlabel=\"Case ID\", ylabel = \"Count\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.suptitle(\"Top 100 cases containing the most citations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "Some cases cite more than a hundred cases. This might be an anomaly. However, it's equally likely that it is also possible and many cases are referred to in a single caselaw. This naturally supports the idea that caselaws build use citations to build strong arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of cases in the processed dataset : {len(cases_data)}\")\n",
    "print(f\"Total number of columns in the processed dataset : {len(cases_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casebody_data.loc[cases_data.index].reset_index().to_json(\"../data/subset/casebody.json\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the graph into [neo4j](https://neo4j.com/) graph database for further analysis. It can be run by running docker and the following command from the project root directory. \n",
    "\n",
    "Copy the subset csv files from the `data/subset` folder to the import folder under `neo4j/import/`\n",
    "Run the following commands.\n",
    "\n",
    "\n",
    "```{bash}\n",
    "docker run \\\n",
    "    --name caselaw-neo4j \\\n",
    "    -p7474:7474 -p7687:7687 \\\n",
    "    -d \\\n",
    "    -v $PWD/neo4j/data:/data \\\n",
    "    -v $PWD/neo4j/logs:/logs \\\n",
    "    -v $PWD/neo4j/import:/var/lib/neo4j/import \\\n",
    "    -v $PWD/neo4j/plugins:/plugins \\\n",
    "    -e NEO4J_apoc_export_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_enabled=true \\\n",
    "    -e NEO4J_apoc_import_file_use__neo4j__config=true \\\n",
    "    --env NEO4J_AUTH='neo4j/zaq!0pl' \\\n",
    "    --env NEO4JLABS_PLUGINS='[\"apoc\", \"graph-data-science\"]' \\\n",
    "    neo4j:latest\n",
    "```{bash}                            \n",
    "\n",
    "```{cypher}                            \n",
    "CREATE CONSTRAINT jurisdictionId IF NOT EXISTS on (jur:Jurisdiction) ASSERT jur.id IS UNIQUE;\n",
    "LOAD CSV WITH HEADERS FROM 'file:///jurisdictions.csv' AS row\n",
    "WITH row\n",
    "MERGE (jurisdiction:Jurisdiction {id:toInteger(row.id)})\n",
    "ON CREATE SET jurisdiction.name = row.name_long,\n",
    "jurisdiction.url = row.url;\n",
    "\n",
    "CREATE CONSTRAINT courtId IF NOT EXISTS on (cou:Court) ASSERT cou.id IS UNIQUE;\n",
    "LOAD CSV WITH HEADERS FROM 'file:///courts.csv' AS row\n",
    "WITH row\n",
    "MERGE (court:Court {id: toInteger(row.id)})\n",
    "ON CREATE SET court.name = row.name,\n",
    "court.url = row.url;\n",
    "\n",
    "CREATE CONSTRAINT reporterId IF NOT EXISTS on (rep:Reporter) ASSERT rep.id IS UNIQUE;\n",
    "LOAD CSV WITH HEADERS FROM 'file:///reporters.csv' AS row\n",
    "WITH row\n",
    "MERGE (reporter:Reporter {id: toInteger(row.id)})\n",
    "ON CREATE SET reporter.name = row.full_name,\n",
    "reporter.url = row.url;\n",
    "\n",
    "CREATE CONSTRAINT volumeId IF NOT EXISTS on (vol:Volume) ASSERT vol.id IS UNIQUE;\n",
    "LOAD CSV WITH HEADERS FROM 'file:///volumes.csv' AS row\n",
    "WITH row\n",
    "MERGE (volume:Volume {id: toInteger(row.volume_number)})\n",
    "ON CREATE SET volume.barcode = row.barcode,\n",
    "volume.url = volume.url;\n",
    "\n",
    "CREATE CONSTRAINT caselawId IF NOT EXISTS on (cas:Caselaw) ASSERT cas.id IS UNIQUE;\n",
    ":auto USING PERIODIC COMMIT 1000\n",
    "LOAD CSV WITH HEADERS FROM 'file:///cases.csv' AS row\n",
    "WITH row\n",
    "MATCH (volume:Volume {id: toInteger(row.volume_id)})\n",
    "MATCH (court:Court {id: toInteger(row.court_id)})\n",
    "MATCH (reporter:Reporter {id: toInteger(row.reporter_id)})\n",
    "MATCH (jurisdiction:Jurisdiction {id: toInteger(row.jurisdiction_id)})\n",
    "MERGE (caselaw:Caselaw {id: toInteger(row.id)})\n",
    "MERGE (caselaw) -[:REPORTED_BY]->(reporter)\n",
    "MERGE (caselaw) -[:HEARD_BY]->(court)\n",
    "MERGE (caselaw) -[:IN_VOLUME]->(volume)\n",
    "MERGE (caselaw) -[:UNDER_JURISIDICTION]->(jurisdiction)\n",
    "on CREATE SET caselaw.decision_date = datetime(row.decision_date),\n",
    "caselaw.name = row.name,\n",
    "caselaw.url = row.url;\n",
    "\n",
    ":auto USING PERIODIC COMMIT 1000\n",
    "LOAD CSV WITH HEADERS FROM 'file:///citations.csv' AS row\n",
    "with row\n",
    "MATCH (case1: Caselaw {id: toInteger(row.src)})\n",
    "MATCH (case2: Caselaw {id: toInteger(row.dst)})\n",
    "MERGE (case1) -[:CITED]-> (case2);\n",
    "\n",
    "```\n",
    "\n",
    "For pre installed database copy the `neo4j` folder from the following drive link https://drive.google.com/drive/folders/1afRp7eMZqIyScYoC1ELdnrKMwfE4YUJh?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample case graph visualization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "print(\"A sample representation of a caselaw in the graph database\")\n",
    "Image(filename='../reports/Sample Case law network.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(password=\"zaq!0pl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the most cited cases in a jurisdiction ?\n",
    "display(graph.run(\"\"\"\n",
    "MATCH g=(j1:Jurisdiction)<-[:UNDER_JURISIDICTION]-(c1:Caselaw)<-[:CITED]-(c2)\n",
    "with j1, c1, count(DISTINCT c2) as citations\n",
    "ORDER BY citations desc\n",
    "WITH j1.name as Jurisdiction, collect(c1.id)[0] as case_id,\n",
    "collect(c1.name)[0] as case_name, collect(citations)[0] as num_citations\n",
    "RETURN Jurisdiction, case_id, case_name, num_citations\n",
    "ORDER BY Jurisdiction ASC\n",
    "\"\"\").to_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(graph.run(\"\"\"MATCH g=(j1:Jurisdiction)<-[:UNDER_JURISIDICTION]-(c1:Caselaw)<-[:CITED]-(c2)\n",
    "with j1, c1, count(DISTINCT c2) as citations\n",
    "ORDER BY citations desc\n",
    "WITH j1.name as Jurisdiction, collect({caselaw: c1.name, citations: citations}) as top_k\n",
    "RETURN Jurisdiction, top_k[0..5]\n",
    "ORDER BY Jurisdiction DESC\n",
    "\"\"\").to_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(graph.run(\"\"\"MATCH (j1:Jurisdiction)<-[:UNDER_JURISIDICTION]-(c1:Caselaw)-[:CITED]-(c2:Caselaw)-[:UNDER_JURISIDICTION]->(j2:Jurisdiction)\n",
    "WHERE j1 <> j2\n",
    "WITH j1, j2, count(DISTINCT c2) as cids\n",
    "ORDER BY cids DESC\n",
    "RETURN j1.name as src_Jurisdiction, collect(j2.name)[0] as dest_Jurisdiction, cids as cited_cases;\"\"\").to_table())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How are cases from one jurisdiction related to another ?*\n",
    "\n",
    "The following query shows a sample of cases that connect different jurisdictions in the graph.\n",
    "\n",
    "```{cypher}\n",
    "MATCH (co1:Court)<-[:HEARD_BY]-(c1:Caselaw)-[:CITED]-(c2:Caselaw)-[:HEARD_BY]->(co2:Court),\n",
    "(c1)-[:UNDER_JURISIDICTION]->(j1:Jurisdiction)\n",
    "WHERE not((c2)-[:UNDER_JURISIDICTION]->(j1))\n",
    "RETURN co1, co2, c1, c2, j1\n",
    "LIMIT 20;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../reports/Cases relating one jurisdiction to another.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about cases citing from courts in other jurisdictions ?\n",
    "\n",
    "The following query allows us to retrive a sample of cases that linked to courts in Arkansas and courts from other Jurisdictions.\n",
    "\n",
    "```\n",
    "MATCH (j1:Jurisdiction)<-[:UNDER_JURISIDICTION]-(c1:Caselaw)-[:CITED]-(c2:Caselaw)-[:UNDER_JURISIDICTION]->(j2:Jurisdiction)\n",
    "WHERE j1 <> j2\n",
    "RETURN j2,c1, c2, j1\n",
    "LIMIT 20;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../reports/sample_jurisdiction court_graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**\n",
    "\n",
    "As we saw the case texts are quite large approx. (28k-50k characters). The texts also contaim misspellings and OCR errors. Additionally, we see hints from reviewing a few sample cases that there are mentions of person names, organizations and other entities such as dates in the case data that might be relevant in idenifying the facts of a case. To preprocess the text, we first concatenate the case text (`Head matter` and `Opinions`) into a single field and proceed to preprocess the text.\n",
    "\n",
    "**Preprocessing Steps**\n",
    " - We parse the text applying POS tagging to the text using [spacy](https://spacy.io/).\n",
    " - We retain only words that are tagged as `Adjectives`, `Adverbs`, `Nouns`, `Proper Nouns` and `Pronouns`.\n",
    " - This reduces the vocabulary size and reduces the dimensionality of the data by a large factor and makes it feasable to work with the text in the dataset for our Exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{python}\n",
    "###\n",
    "### CAUTION TAKE REALLY LONG TIME TO RUN\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "casetext = casebody_data[\"head_matter\"] + \"\\n\"+ casebody_data[\"opinions\"].map(lambda x: \" \".join(y.get(\"text\", \"\") for y in x))\n",
    "with open(\"../data/subset/caselines.txt\", \"w+\") as outfile:\n",
    "    tags = frozenset([\"ADJ\",\"ADV\",\"NOUN\",\"PRON\",\"PROPN\"])\n",
    "    for doc in tqdm.tqdm_notebook(nlp.pipe(casetext,batch_size=10, n_process=-1), total=len(casetext)):\n",
    "        tokens = []\n",
    "        for tok in doc:\n",
    "            if tok.pos_ in tags and tok.is_alpha:\n",
    "                tokens.append(tok.lemma_.lower())\n",
    "        tokens = \" \".join(tokens)\n",
    "        tokens = preprocess_string(tokens,DEFAULT_FILTERS[:-1])\n",
    "        outline = \" \".join(tokens) + \"\\n\"\n",
    "        outfile.write(outline)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_lines = pd.read_csv(\"../data/subset/caselines.txt\", header=None, names=[\"text\"])\n",
    "case_lines.index=cases_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cases_data = cases_data[[\"head_matter\", \"jurisdiction_id\", \"court_id\", \"name_abbreviation\", \"citation_ids\"]]\n",
    "cases_data[\"citation_ids\"] = cases_data.citation_ids.map(lambda x: [int(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import option_context\n",
    "# sample preprocessed text\n",
    "with option_context('display.max_colwidth', 50):\n",
    "    display(case_lines.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst):\n",
    "    return (item for sublist in lst for item in sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_vocab = {}\n",
    "groups = cases_data.groupby(\"jurisdiction_id\").groups.items()\n",
    "for grp,idx in groups:\n",
    "    grp_data = case_lines.loc[idx]\n",
    "    top_10 = Counter(flatten(grp_data.text.str.split().tolist())).most_common(10)\n",
    "    grp_vocab[grp] = top_10\n",
    "num_plots = len(grp_vocab)\n",
    "fig, ax = plt.subplots(nrows=num_plots, ncols=1, figsize=(20, 20))\n",
    "for i, (k,v) in enumerate(grp_vocab.items()):\n",
    "    title = jurisdictions_data[jurisdictions_data.id == k]['name_long'].values[0]\n",
    "    word, count = zip(*v)\n",
    "    sns.barplot(x=np.array(word),y=np.array(count),ax=ax[i])\n",
    "    ax[i].set_title(f\"Jurisdiction:{title}\")\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.suptitle(\"Top 10 words by Jurisdiction\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grp_vocab = {}\n",
    "groups = cases_data.groupby(\"court_id\").groups.items()\n",
    "for grp,idx in groups:\n",
    "    grp_data = case_lines.loc[idx]\n",
    "    top_10 = Counter(flatten(grp_data.text.str.split().tolist())).most_common(10)\n",
    "    grp_vocab[grp] = top_10\n",
    "num_plots = len(grp_vocab)\n",
    "fig, ax = plt.subplots(nrows=num_plots, ncols=1, figsize=(20, 40))\n",
    "for i, (k,v) in enumerate(grp_vocab.items()):\n",
    "    title = courts_data[courts_data.id == k]['name'].values[0]\n",
    "    word, count = zip(*v)\n",
    "    sns.barplot(x=np.array(word),y=np.array(count),ax=ax[i])\n",
    "    ax[i].set_title(f\"Court:{title}\")\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.suptitle(\"Top 10 words by Court\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "- Most of the top words are the same among the Jurisdictions and include: \n",
    "- We observe a small variation among the top words in the Courts For instance `court of appeals`  and `appallette courts` frequently feature the terms such as `defendent` and `appellent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_data[\"year\"] = cases_data['decision_date'].dt.year\n",
    "\n",
    "year_vocab = {}\n",
    "year = [1813,1830,1870,1910,1947,1950,1990, 2000,2010, 2018]\n",
    "groups = cases_data.groupby(\"year\").groups.items()\n",
    "for grp,idx in groups :\n",
    "    if grp in year:\n",
    "        grp_data = case_lines.loc[idx]\n",
    "        top_5 = Counter(flatten(grp_data.text.str.split().tolist())).most_common(5)\n",
    "        year_vocab[grp] = top_5\n",
    "    \n",
    "num_plots = len(year_vocab)\n",
    "fig, ax = plt.subplots(nrows=num_plots, ncols=1, figsize=(20, 30))\n",
    "for i, (k,v) in enumerate(year_vocab.items()):\n",
    "    title = k\n",
    "    word, count = zip(*v)\n",
    "    sns.barplot(x=np.array(word),y=np.array(count),ax=ax[i])\n",
    "    ax[i].set_title(f\"\\n\\nYear:{title}\" , fontsize=20)\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.subplots_adjust(hspace = 0.8)\n",
    "plt.suptitle(\"Top 5 words by Year\", fontsize=30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A Simple Baseline: Text Similarity for recommending citations.**\n",
    "\n",
    "We hypothesize that cited caselaws often share similar vocabulary with the citing case text. This motivate us to explore a similarity metrics such since cosine similarity among document vectors to retrieve relevant documents given a query document.\n",
    "\n",
    "**Methodology**\n",
    "\n",
    "- We convert the preprocessed text to a bag-of-words representation. \n",
    "- The vocabulary contains ~538954 words. \n",
    "- We reduce the dimensionality of the data by pruning the vocabulary to retain the 50000 most common words. \n",
    "- Next we create TFIDF vectors of the documents and measure the cosine similarity of the tfidf vectors.\n",
    "- We then compute the mean cosine similarity of a sample of documents in the corpus with their citations to answer the question- `How similar are cited cases to each other ?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CaselineIterator:\n",
    "#     def __init__(self, fname, slice_size=None):\n",
    "#         self.fname = fname\n",
    "#         self.slice_size=slice_size\n",
    "#         self._length = slice_size\n",
    "#     def __iter__(self):\n",
    "#         if self.slice_size:\n",
    "#             iterator = islice(open(self.fname), self.slice_size)\n",
    "#         else:\n",
    "#             iterator = open(self.fname)\n",
    "#         for line in iterator:\n",
    "#             yield line.split()\n",
    "            \n",
    "#     def __len__(self):\n",
    "#         if not self._length:\n",
    "#             self._length = sum(1 for _ in open(self.fname))\n",
    "#         return self._length\n",
    "\n",
    "class CaselawCorpus():\n",
    "    def __init__(self, iterator, dictionary, **kwargs):\n",
    "        self.iterator = iterator\n",
    "        self.dictionary = dictionary\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for line in self.iterator:\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield self.dictionary.doc2bow(line)\n",
    "    def __len__(self):\n",
    "        return len(self.iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSimilariyIndexer:\n",
    "    def __init__(self, no_below=5, no_above=0.75):\n",
    "        self.dictionary = None\n",
    "        self.corpus = None\n",
    "        self.model = None\n",
    "        self.index = None\n",
    "        self.no_below=no_below\n",
    "        self.no_above=no_above\n",
    "    \n",
    "    def fit(self, texts, **kwargs):\n",
    "        self.dictionary = Dictionary(texts)\n",
    "        self.dictionary.filter_extremes(self.no_below, self.no_above, keep_n=50000)\n",
    "        self.dictionary.compactify()\n",
    "        \n",
    "        self.corpus = CaselawCorpus(texts, self.dictionary,**kwargs)\n",
    "        self.model = TfidfModel(self.corpus)\n",
    "        self.index = SparseMatrixSimilarity(\n",
    "            self.model[self.corpus], \n",
    "            num_features=len(self.dictionary),\n",
    "            num_terms=len(self.dictionary),\n",
    "            num_docs=len(self.corpus),\n",
    "            maintain_sparsity=True)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, texts, top_k=5):\n",
    "        pred_corpus = CaselawCorpus(texts, self.dictionary)\n",
    "        pred_vecs = self.model[pred_corpus]\n",
    "        pred_sims = self.index[pred_vecs]\n",
    "        return pred_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_indexer = CosineSimilariyIndexer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cases_data, case_lines, casebody_data, citation_graph, citations_data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "case_info = cases_data[[\"jurisdiction_id\", \"court_id\", \"decision_date\", \"head_matter\", \"opinion_text\", \"citation_ids\"]] "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "case_info = case_info.reset_index()\n",
    "case_info.to_json(\"../data/subset/case_info.json\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_data = pd.read_json(\"../data/subset/case_info.json\", lines=True, orient=\"records\")\n",
    "case_lines = pd.read_csv(\"../data/subset/caselines.txt\", header=None, names=[\"text\"])\n",
    "case_lines.index=cases_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2cases = pd.Series(cases_data.index, index=pd.Index(cases_data[\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a lookup for our cases\n",
    "citations_uids = frozenset(cases_data[\"id\"])\n",
    "\n",
    "# remove citations that aren't in case data\n",
    "cases_data[\"citation_ids\"] = cases_data.citation_ids.map(lambda x: list(filter(lambda y: y in citations_uids, x)))\n",
    "cases_data[\"citation_idxs\"] = cases_data.citation_ids.map(lambda x: id2cases[x].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similarity_indexer.fit(case_lines.text.str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we sample a few cases to validate our hypothesis related to text similarity\n",
    "# note this is still training set and not a dev or test set\n",
    "sample_case_lines = case_lines.sample(1000)\n",
    "with pd.option_context('display.max_colwidth', 400):\n",
    "    display(sample_case_lines.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get similairty of queries to corpus\n",
    "sample_preds = similarity_indexer.predict(sample_case_lines.text.str.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions into dense matrix\n",
    "sample_preds = sample_preds.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean similarities of citations and their corresponding documents\n",
    "all_sims = []\n",
    "for item in tqdm.tqdm_notebook(cases_data.loc[sample_case_lines.index].citation_idxs):\n",
    "    for idx, sim in zip(item, sample_preds):\n",
    "        \n",
    "        mean_sim = sim[idx].mean()\n",
    "        all_sims.append(mean_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean cosine similarity of a sample of 1000 cases to their citations: {np.mean(all_sims):2.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    " - Very low mean similarities among cited documents\n",
    " - This is perhaps due to the high dimensionality of data.\n",
    " - TFIDF doesn't capture other factors such as jurisdiction, court and date in similarity calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A simple heuristic to improve our baseline**\n",
    "\n",
    "Since we know that cases are more often cited among Jurisdictions and courts we use this to arrive at a baseline when used alongside our cosine similarity index. We take the following approach during inference in out baseline:\n",
    "\n",
    "While predicting the recommendations include only documents from the corpus that have:\n",
    "    - the same jurisdiction_id\n",
    "    - the same court_id and\n",
    "    - has a data before the query document date.\n",
    "    \n",
    "These assumptions are valid even for documents that are quried for citations recommendations. i.e. even for unknown documents we know what `court` and `jurisdiction` a case if being decided at when referring to a caselaw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "top_preds= []\n",
    "for (idx, row), sample_pred in tqdm.tqdm_notebook(zip(cases_data.loc[sample_case_lines.index].iterrows(), sample_preds)):\n",
    "    row = row.to_dict()\n",
    "    \n",
    "    jur_id = row[\"jurisdiction_id\"]\n",
    "    court_id = row[\"court_id\"]\n",
    "    decision_date = row[\"decision_date\"]\n",
    "    sample_pred = sample_pred.argsort()[::-1]\n",
    "    pred = sample_pred[\n",
    "        (cases_data.jurisdiction_id == jur_id) &\\\n",
    "        (cases_data.court_id == court_id) &\\\n",
    "        (cases_data.decision_date > decision_date)]\n",
    "    top_preds.append(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_k(y_true, y_pred, k=10):\n",
    "    precisions =[]\n",
    "    recalls = []\n",
    "    for y_t, y_p in zip(y_true, y_pred):\n",
    "        y_p = y_p[:k]\n",
    "        relevant_retrieved = set(y_t).intersection(y_p)\n",
    "        if relevant_retrieved:\n",
    "            precision = len(relevant_retrieved)/len(y_p)\n",
    "            recall = len(relevant_retrieved)/len(y_t)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "    return np.mean(precisions), np.mean(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisions =[]\n",
    "recalls = []\n",
    "for i in range(3,28, 3):\n",
    "    precision, recall =  precision_recall_k(cases_data.loc[sample_case_lines.index].citation_idxs.values, top_preds, i)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_plot_df = pd.DataFrame(zip(precisions, recalls), columns=[\"precision\", \"recall\"], index=range(3,30,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.lineplot(data=pk_plot_df, markers=True)\n",
    "plt.title(\"top-k Precision and recall for recommended samples from baseline model\")\n",
    "plt.xlabel(\"top-k\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "- Not much gain in recall even as we increase the number of retrieved documents.\n",
    "- Precision drops very steeply after 5 documents.\n",
    "- Top-5 results seem to have the best precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions and Next steps \n",
    "\n",
    "- How do we build a train and test set when we need know they overlap in terms of cited and citing documents ?\n",
    "- It looks like a simple information retereival approch is not enough when building caselaw recommendation systems.\n",
    "- How do we represent the global and local context of a citation when making a recommendations ?\n",
    "- Can we exploit the graph structure to derive secondary references of citations to allow for deeper research in recommendations ?\n",
    "- Explore building graph based embeddings that can be used to rerank the retrieved results by only use text features as input.\n",
    "- Explore polarity of citations based on opinion `dissent`,`majority` and `concurrent` types of opinions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split and Mapping Citiations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_fname = \"../data/subset/case_info.json\"\n",
    "cases_df = pd.read_json(base_fname, lines=True, orient=\"records\")\n",
    "cases_df[\"decision_date\"] = pd.to_datetime(cases_df.decision_date, unit='ms')\n",
    "citations_uids = frozenset(cases_df[\"id\"])\n",
    "cases_df[\"citation_ids\"] = cases_df.citation_ids.map(lambda x: list(filter(lambda y: y in citations_uids, x)))\n",
    "cases_df = cases_df[cases_df.citation_ids.map(len) > 0]\n",
    "cases_df = cases_df.sort_values([\"decision_date\", \"id\"])\n",
    "cases_df = cases_df.reset_index(drop=True)\n",
    "train_df = cases_df.iloc[:int(len(cases_df) * .8)]\n",
    "test_df = cases_df.loc[cases_df.index.difference(train_df.index)]\n",
    "val_df = test_df.iloc[:int(len(test_df) * .5)]\n",
    "test_df = test_df.loc[test_df.index.difference(val_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_json(\"../data/subset/train_data.json\", lines=True, orient=\"records\")\n",
    "val_df.to_json(\"../data/subset/val_data.json\", lines=True, orient=\"records\")\n",
    "test_df.to_json(\"../data/subset/test_data.json\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df = pd.read_json(\"../data/subset/case_info.json\", lines=True, orient=\"records\")[[\"id\"]]\n",
    "train_df = pd.read_json(\"../data/subset/train_data.json\", lines=True, orient=\"records\")[[\"id\", \"citation_ids\"]]\n",
    "val_df = pd.read_json(\"../data/subset/val_data.json\", lines=True, orient=\"records\")[[\"id\", \"citation_ids\"]]\n",
    "test_df = pd.read_json(\"../data/subset/test_data.json\", lines=True, orient=\"records\")[[\"id\", \"citation_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_uids = frozenset(cases_df['id'].values)\n",
    "\n",
    "cite2idx = {k:v for v,k in cases_df.id.items()}\n",
    "idx2cite = {k:v for v,k in cite2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(cite2idx, open(\"../data/subset/cite2idx.json\", \"w+\"))\n",
    "json.dump(idx2cite, open(\"../data/subset/idx2cite.json\", \"w+\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pos_neg_ids(df, citation_uids, cite2idx):\n",
    "    df.loc[:, \"pos_citations\"] = df.citation_ids.map(lambda x: [y for y in x if y in citation_uids])\n",
    "    df.loc[:, \"neg_citations\"] = df.pos_citations.map(lambda x: random.sample(citation_uids.difference(x), len(x) if x else 1)) \n",
    "    df.loc[:, \"pos_citations\"] = df.pos_citations.map(lambda x: [cite2idx[y] for y in x] if x else x)\n",
    "    df.loc[:, \"neg_citations\"] = df.neg_citations.map(lambda x: [cite2idx[y] for y in x] if x else x)\n",
    "    df.loc[:, 'id'] = df.id.map(cite2idx.get)\n",
    "    pos_citations = df[[\"id\", \"pos_citations\"]].explode(\"pos_citations\").rename({\"pos_citations\": \"citation\"}, axis=1)\n",
    "    pos_citations[\"label\"] = 0\n",
    "    neg_citations = df[[\"id\", \"neg_citations\"]].explode(\"neg_citations\").rename({\"neg_citations\": \"citation\"}, axis=1)\n",
    "    neg_citations[\"label\"] = 1\n",
    "    df = pd.concat([pos_citations, neg_citations]).sample(frac=1, replace=False).dropna().astype(np.int32)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_pos_neg_ids(train_df, citation_uids, cite2idx)\n",
    "val_df = load_pos_neg_ids(val_df, citation_uids, cite2idx)\n",
    "test_df = load_pos_neg_ids(test_df, citation_uids, cite2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"../data/subset/train_map.csv\", index=False, index_label=False)\n",
    "val_df.to_csv(\"../data/subset/val_map.csv\", index=False, index_label=False)\n",
    "test_df.to_csv(\"../data/subset/test_map.csv\", index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets Embeddings and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import datasets\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import normalize\n",
    "import faiss\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Delcare filepaths\n",
    "\n",
    "#dataset directory\n",
    "base_data_dir = \"../data/subset\"\n",
    "\n",
    "#base dataset\n",
    "data_fname = f\"{base_data_dir}/case_info.json\"\n",
    "\n",
    "#embeddings dataset\n",
    "embeddings_model_name=\"allenai/specter\"\n",
    "embedding_type = \"specter_pooled\"\n",
    "embedding_dataset_dir = f\"{base_data_dir}/{embedding_type}_embeddings_dataset\"\n",
    "\n",
    "#classification dataset\n",
    "clf_dataset_dir = f\"{base_data_dir}/{embedding_type}_clf_dataset\"\n",
    "data_files = {\n",
    "    \"train\": f\"{base_data_dir}/train_map.csv\", \n",
    "    \"validation\": f\"{base_data_dir}/val_map.csv\", \n",
    "    \"test\": f\"{base_data_dir}/test_map.csv\",}\n",
    "\n",
    "#model\n",
    "models_dir = \"../models\"\n",
    "model_name = \"specter_pooled_clf_model\"\n",
    "model_checkpoint = f\"{models_dir}/{model_name}\"\n",
    "model_log_fname = f\"{models_dir}/logs/{model_name}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(examples):\n",
    "    \"\"\"Create text features from head matter and opinion text for a batch of examples\"\"\"\n",
    "    \n",
    "    batch = [item[0] + \"\\n\" + item[1] for item in zip(examples['head_matter'], examples['opinion_text'])]\n",
    "    return {\"text\": batch}\n",
    "\n",
    "def load_embeddings(examples, tokenizer, model, embedding_type):\n",
    "    \"\"\"Tokenize and load embeddings from given huggingface pretrained model\"\"\"\n",
    "    \n",
    "    tokenized = tokenizer(examples[\"text\"],\n",
    "                          return_tensors=\"tf\",\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          max_length=512\n",
    "                         )\n",
    "    if embedding_type == \"pooled\":\n",
    "        embeddings = {'embeddings': normalize(model(**tokenized)[1].numpy())}\n",
    "    else:\n",
    "        embeddings = {'embeddings': normalize(model(**tokenized)[0][:,0:].numpy())}\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def load_embeddings_dataset(\n",
    "    dataset_dir, embedding_type, embedding_model=\"allenai/specter\",\n",
    "    num_proc=15, batch_size=256, faiss_device=0):\n",
    "    \"\"\"Load embeddings dataset and create faiss index on embeddings column\"\"\"\n",
    "    \n",
    "    if os.path.isfile(f\"{dataset_dir}/state.json\"):\n",
    "        print(\"Found existing embeddings. loading from disk ...\")\n",
    "        dataset = datasets.Dataset.load_from_disk(dataset_dir)\n",
    "        if os.path.isfile(f\"{dataset_dir}/embeddings.faiss\"):\n",
    "            print(\"Found existing fiass index. loading from disk ...\")\n",
    "            dataset.load_faiss_index(\"embeddings\", f\"{dataset_dir}/embeddings.faiss\")\n",
    "        else:\n",
    "            print(\"No fiass index found. creating and saving new index to disk ...\")\n",
    "            dataset.add_faiss_index(column=\"embeddings\", device=faiss_device, metric_type=faiss.METRIC_INNER_PRODUCT)\n",
    "            dataset.save_faiss_index(\"embeddings\", f\"{dataset_dir}/embeddings.faiss\")\n",
    "    else:\n",
    "        print(\"No existing embeddings found. Creating and saving to disk ...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
    "        model = TFAutoModel.from_pretrained(embedding_model, from_pt=True)\n",
    "        \n",
    "        print(\"Loading dataset and text column ...\")\n",
    "        dataset = datasets.load_dataset(\"json\", data_files=data_fname, split=datasets.splits.Split(\"train\"))\n",
    "        exclude_columns = [\"jurisdiction_id\",\"court_id\",\"decision_date\", \"head_matter\",\"opinion_text\",\"citation_ids\"]\n",
    "        dataset = dataset.map(load_text, batched=True, num_proc=num_proc, remove_columns=exclude_columns)\n",
    "        print(\"Loading embeddings ...\")\n",
    "        embedder = partial(load_embeddings, tokenizer=tokenzier, model=model, embedding_type=embedding_type.split(\"_\")[-1])\n",
    "        dataset = dataset.map(embedder, batched=True, batch_size=batch_size)\n",
    "        print(f\"Saving Dataset to disk at {dataset_dir}\")\n",
    "        dataset.save_to_disk(dataset_dir)\n",
    "        print(\"Creating new fiass index and saving to disk ...\")\n",
    "        dataset.add_faiss_index(column=\"embeddings\", device=faiss_device, metric_type=faiss.METRIC_INNER_PRODUCT)\n",
    "        dataset.save_faiss_index(\"embeddings\", f\"{dataset_dir}/embeddings.faiss\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dataset = load_embeddings_dataset(\n",
    "    embedding_dataset_dir,\n",
    "    embedding_type,\n",
    "    embedding_model=embeddings_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clf_embeddings(examples):\n",
    "    \"\"\"load embeddings into classification dataset.\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"case_embedding\": embeddings_dataset[examples[\"id\"]][\"embeddings\"],\n",
    "        \"citation_embedding\": embeddings_dataset[examples[\"citation\"]][\"embeddings\"],\n",
    "        }\n",
    "\n",
    "\n",
    "def generator_from_dataset(dataset):\n",
    "    \"\"\"Create a generator from a huggignface dataset.\"\"\"\n",
    "    \n",
    "    def _gen():\n",
    "        for item in dataset:\n",
    "            features = (item[\"case_embedding\"], item[\"citation_embedding\"])\n",
    "            yield features, item['label']\n",
    "    return _gen\n",
    "\n",
    "\n",
    "def tf_dataset_from_dataset(dataset):\n",
    "    \"\"\"Create a tensorflow dataset from a huggingface dataset using a generator.\"\"\"\n",
    "    \n",
    "    dataset_generator = generator_from_dataset(dataset)\n",
    "    tfdataset = tf.data.Dataset.from_generator(\n",
    "        dataset_generator,\n",
    "        output_signature=(\n",
    "         (tf.TensorSpec(shape=(768,), dtype=tf.float32),\n",
    "          tf.TensorSpec(shape=(768,), dtype=tf.float32),\n",
    "         ),\n",
    "         tf.TensorSpec(shape=(None), dtype=tf.int32))\n",
    "    )\n",
    "    tfdataset = tfdataset.apply(tf.data.experimental.assert_cardinality(len(dataset)))\n",
    "    return tfdataset\n",
    "\n",
    "def shuffle_batch_repeat(dataset, batch_size=64, shuffle=False):\n",
    "    \"\"\"Batch shuffle and repeat a tensorflow dataset infinitely.\"\"\"\n",
    "    \n",
    "    dataset = dataset.repeat()\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size*4)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_clf_datasets(\n",
    "    dataset_dir, data_files, batch_size=64,\n",
    "    embeddings_dataset=None, num_proc=15):\n",
    "    \"\"\"Create tensorflow dataset with pairs for embeddings and a label for classification.\"\"\"\n",
    "    \n",
    "    if os.path.isfile(f\"{dataset_dir}/dataset_dict.json\"):\n",
    "        print(\"Found existing dataset dict. loading from disk ...\")\n",
    "        dataset = datasets.DatasetDict.load_from_disk(dataset_dir, keep_in_memory=True)\n",
    "        dataset.set_format(type='tensorflow', columns=['case_embedding', 'citation_embedding', 'label'])\n",
    "        \n",
    "        train_dataset = dataset['train']\n",
    "        val_dataset = dataset['validation']\n",
    "        test_dataset = dataset['test']\n",
    "        \n",
    "    elif embeddings_dataset is not None:\n",
    "        print(\"Found data files and embeddings dataset. Creating new clf dataset and saving to disk ...\")\n",
    "        dataset = datasets.load_dataset(\"csv\", data_files=data_files)\n",
    "        print(\"Mapping embeddings to clf dataset ...\")        \n",
    "        dataset = dataset.map(load_clf_embeddings, batched=True, num_proc=num_proc)\n",
    "        print(f\"Saving Dataset to disk at {dataset_dir}\")\n",
    "        dataset.save_to_disk(dataset_dir)\n",
    "        dataset.set_format(type='tensorflow', columns=['case_embedding', 'citation_embedding', 'label'])\n",
    "        \n",
    "        train_dataset = dataset['train']\n",
    "        val_dataset = dataset['validation']\n",
    "        test_dataset = dataset['test']\n",
    "\n",
    "    print(\"loading tensorflow datasets ...\")\n",
    "    train_dataset = tf_dataset_from_dataset(train_dataset)\n",
    "    val_dataset = tf_dataset_from_dataset(val_dataset)\n",
    "    test_dataset = tf_dataset_from_dataset(test_dataset)\n",
    "    \n",
    "    print(\"batching and shuffling tensorflow datasets ... \")\n",
    "    train_dataset = shuffle_batch_repeat(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = shuffle_batch_repeat(val_dataset, batch_size=batch_size,)\n",
    "    test_dataset = shuffle_batch_repeat(test_dataset, batch_size=batch_size,)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, hidden_dim=256, dropout=0.2):\n",
    "    \"\"\"Create a classification model for paired embedding inputs\"\"\"\n",
    "    \n",
    "    case_input = tf.keras.layers.Input(shape=(768,), dtype=tf.float32, name=\"case_input\")\n",
    "    citation_input = tf.keras.layers.Input(shape=(768,), dtype=tf.float32, name=\"citation_input\")\n",
    "\n",
    "    case_representation = tf.keras.layers.BatchNormalization()(case_input)\n",
    "    citation_representation = tf.keras.layers.BatchNormalization()(citation_input)\n",
    "\n",
    "    case_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(case_representation)\n",
    "\n",
    "    citation_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(citation_representation)\n",
    "\n",
    "    case_representation = tf.keras.layers.Dropout(dropout,)(case_representation)\n",
    "    citation_representation = tf.keras.layers.Dropout(dropout,)(citation_representation)\n",
    "\n",
    "    case_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(case_representation)\n",
    "    citation_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(citation_representation)\n",
    "\n",
    "    concatenated = tf.keras.layers.Concatenate()([case_representation, citation_representation])\n",
    "\n",
    "    def shared_stack(prev_input):\n",
    "        return tf.keras.models.Sequential(\n",
    "            [tf.keras.layers.BatchNormalization(),\n",
    "             tf.keras.layers.Dense(hidden_dim, activation=\"relu\"),\n",
    "             tf.keras.layers.Dropout(dropout)\n",
    "            ])(prev_input) \n",
    "    concatenated = shared_stack(concatenated)\n",
    "    concatenated = shared_stack(concatenated)\n",
    "    concatenated = shared_stack(concatenated)\n",
    "    output = tf.keras.layers.Dense(1, dtype=tf.float32, activation=\"sigmoid\")(concatenated)\n",
    "    clf_model = tf.keras.models.Model(inputs=[case_input, citation_input], outputs=[output], name=model_name)\n",
    "    loss= tf.keras.losses.BinaryCrossentropy()\n",
    "    acc = tf.keras.metrics.BinaryAccuracy()\n",
    "    clf_model.compile(loss=loss, metrics=[acc],  optimizer=\"adam\")\n",
    "    clf_model.summary()\n",
    "    return clf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "             tf.keras.callbacks.ModelCheckpoint(model_checkpoint, save_best_only=True,),\n",
    "             tf.keras.callbacks.CSVLogger(model_log_fname),\n",
    "             tf.keras.callbacks.ReduceLROnPlateau(patience=3, verbose=1)\n",
    "            ]\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset, val_dataset, test_dataset = load_clf_datasets(\n",
    "    clf_dataset_dir, data_files, batch_size=128, embeddings_dataset=embeddings_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = clf_model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=1000,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=100,\n",
    "    verbose=1,\n",
    "    epochs=25,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "clf_model.save(model_checkpoint) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
