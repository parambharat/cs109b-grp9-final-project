{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import datasets\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import normalize\n",
    "import faiss\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "from tensorflow.keras import mixed_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA A100-PCIE-40GB, compute capability 8.0\n"
     ]
    }
   ],
   "source": [
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Delcare filepaths\n",
    "\n",
    "#dataset directory\n",
    "base_data_dir = \"../data/subset\"\n",
    "\n",
    "#base dataset\n",
    "data_fname = f\"{base_data_dir}/case_info.json\"\n",
    "\n",
    "#embeddings dataset\n",
    "embeddings_model_name=\"allenai/specter\"\n",
    "embedding_type = \"specter_pooled\"\n",
    "embedding_dataset_dir = f\"{base_data_dir}/{embedding_type}_embeddings_dataset\"\n",
    "\n",
    "#classification dataset\n",
    "clf_dataset_dir = f\"{base_data_dir}/{embedding_type}_clf_dataset\"\n",
    "data_files = {\n",
    "    \"train\": f\"{base_data_dir}/train_map.csv\", \n",
    "    \"validation\": f\"{base_data_dir}/val_map.csv\", \n",
    "    \"test\": f\"{base_data_dir}/test_map.csv\",}\n",
    "\n",
    "#model\n",
    "models_dir = \"../models\"\n",
    "model_name = \"specter_pooled_clf_model\"\n",
    "model_checkpoint = f\"{models_dir}/model_name\"\n",
    "model_log_fname = f\"{models_dir}/logs/{model_name}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(examples):\n",
    "    \"\"\"Create text features from head matter and opinion text for a batch of examples\"\"\"\n",
    "    \n",
    "    batch = [item[0] + \"\\n\" + item[1] for item in zip(examples['head_matter'], examples['opinion_text'])]\n",
    "    return {\"text\": batch}\n",
    "\n",
    "def load_embeddings(examples, tokenizer, model, embedding_type):\n",
    "    \"\"\"Tokenize and load embeddings from given huggingface pretrained model\"\"\"\n",
    "    \n",
    "    tokenized = tokenizer(examples[\"text\"],\n",
    "                          return_tensors=\"tf\",\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          max_length=512\n",
    "                         )\n",
    "    if embedding_type == \"pooled\":\n",
    "        embeddings = {'embeddings': normalize(model(**tokenized)[1].numpy())}\n",
    "    else:\n",
    "        embeddings = {'embeddings': normalize(model(**tokenized)[0][:,0:].numpy())}\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def load_embeddings_dataset(\n",
    "    dataset_dir, embedding_type, embedding_model=\"allenai/specter\",\n",
    "    num_proc=15, batch_size=256, faiss_device=0):\n",
    "    \"\"\"Load embeddings dataset and create faiss index on embeddings column\"\"\"\n",
    "    \n",
    "    if os.path.isfile(f\"{dataset_dir}/state.json\"):\n",
    "        print(\"Found existing embeddings. loading from disk ...\")\n",
    "        dataset = datasets.Dataset.load_from_disk(dataset_dir)\n",
    "        if os.path.isfile(f\"{dataset_dir}/embeddings.faiss\"):\n",
    "            print(\"Found existing fiass index. loading from disk ...\")\n",
    "            dataset.load_faiss_index(\"embeddings\", f\"{dataset_dir}/embeddings.faiss\")\n",
    "        else:\n",
    "            print(\"No fiass index found. creating and saving new index to disk ...\")\n",
    "            dataset.add_faiss_index(column=\"embeddings\", device=faiss_device, metric_type=faiss.METRIC_INNER_PRODUCT)\n",
    "            dataset.save_faiss_index(\"embeddings\", f\"{dataset_dir}/embeddings.faiss\")\n",
    "    else:\n",
    "        print(\"No existing embeddings found. Creating and saving to disk ...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
    "        model = TFAutoModel.from_pretrained(embedding_model, from_pt=True)\n",
    "        \n",
    "        print(\"Loading dataset and text column ...\")\n",
    "        dataset = datasets.load_dataset(\"json\", data_files=data_fname, split=datasets.splits.Split(\"train\"))\n",
    "        exclude_columns = [\"jurisdiction_id\",\"court_id\",\"decision_date\", \"head_matter\",\"opinion_text\",\"citation_ids\"]\n",
    "        dataset = dataset.map(load_text, batched=True, num_proc=num_proc, remove_columns=exclude_columns)\n",
    "        print(\"Loading embeddings ...\")\n",
    "        embedder = partial(load_embeddings, tokenizer=tokenzier, model=model, embedding_type=embedding_type.split(\"_\")[-1])\n",
    "        dataset = dataset.map(embedder, batched=True, batch_size=batch_size)\n",
    "        print(f\"Saving Dataset to disk at {dataset_dir}\")\n",
    "        dataset.save_to_disk(dataset_dir)\n",
    "        print(\"Creating new fiass index and saving to disk ...\")\n",
    "        dataset.add_faiss_index(column=\"embeddings\", device=faiss_device, metric_type=faiss.METRIC_INNER_PRODUCT)\n",
    "        dataset.save_faiss_index(\"embeddings\", f\"{dataset_dir}/embeddings.faiss\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing embeddings. loading from disk ...\n",
      "Found existing fiass index. loading from disk ...\n"
     ]
    }
   ],
   "source": [
    "embeddings_dataset = load_embeddings_dataset(\n",
    "    embedding_dataset_dir,\n",
    "    embedding_type,\n",
    "    embedding_model=embeddings_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clf_embeddings(examples):\n",
    "    \"\"\"load embeddings into classification dataset.\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"case_embedding\": embeddings_dataset[examples[\"id\"]][\"embeddings\"],\n",
    "        \"citation_embedding\": embeddings_dataset[examples[\"citation\"]][\"embeddings\"],\n",
    "        }\n",
    "\n",
    "\n",
    "def generator_from_dataset(dataset):\n",
    "    \"\"\"Create a generator from a huggignface dataset.\"\"\"\n",
    "    \n",
    "    def _gen():\n",
    "        for item in dataset:\n",
    "            features = (item[\"case_embedding\"], item[\"citation_embedding\"])\n",
    "            yield features, item['label']\n",
    "    return _gen\n",
    "\n",
    "\n",
    "def tf_dataset_from_dataset(dataset):\n",
    "    \"\"\"Create a tensorflow dataset from a huggingface dataset using a generator.\"\"\"\n",
    "    \n",
    "    dataset_generator = generator_from_dataset(dataset)\n",
    "    tfdataset = tf.data.Dataset.from_generator(\n",
    "        dataset_generator,\n",
    "        output_signature=(\n",
    "         (tf.TensorSpec(shape=(768,), dtype=tf.float32),\n",
    "          tf.TensorSpec(shape=(768,), dtype=tf.float32),\n",
    "         ),\n",
    "         tf.TensorSpec(shape=(None), dtype=tf.int32))\n",
    "    )\n",
    "    tfdataset = tfdataset.apply(tf.data.experimental.assert_cardinality(len(dataset)))\n",
    "    return tfdataset\n",
    "\n",
    "def shuffle_batch_repeat(dataset, batch_size=64, shuffle=False):\n",
    "    \"\"\"Batch shuffle and repeat a tensorflow dataset infinitely.\"\"\"\n",
    "    \n",
    "    dataset = dataset.repeat()\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size*4)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_clf_datasets(\n",
    "    dataset_dir, data_files, batch_size=64,\n",
    "    embeddings_dataset=None, num_proc=15):\n",
    "    \"\"\"Create tensorflow dataset with pairs for embeddings and a label for classification.\"\"\"\n",
    "    \n",
    "    if os.path.isfile(f\"{dataset_dir}/dataset_dict.json\"):\n",
    "        print(\"Found existing dataset dict. loading from disk ...\")\n",
    "        dataset = datasets.DatasetDict.load_from_disk(dataset_dir, keep_in_memory=True)\n",
    "        dataset.set_format(type='tensorflow', columns=['case_embedding', 'citation_embedding', 'label'])\n",
    "        \n",
    "        train_dataset = dataset['train']\n",
    "        val_dataset = dataset['validation']\n",
    "        test_dataset = dataset['test']\n",
    "        \n",
    "    elif embeddings_dataset is not None:\n",
    "        print(\"Found data files and embeddings dataset. Creating new clf dataset and saving to disk ...\")\n",
    "        dataset = datasets.load_dataset(\"csv\", data_files=data_files)\n",
    "        print(\"Mapping embeddings to clf dataset ...\")        \n",
    "        dataset = dataset.map(load_clf_embeddings, batched=True, num_proc=num_proc)\n",
    "        print(f\"Saving Dataset to disk at {dataset_dir}\")\n",
    "        dataset.save_to_disk(dataset_dir)\n",
    "        dataset.set_format(type='tensorflow', columns=['case_embedding', 'citation_embedding', 'label'])\n",
    "        \n",
    "        train_dataset = dataset['train']\n",
    "        val_dataset = dataset['validation']\n",
    "        test_dataset = dataset['test']\n",
    "\n",
    "    print(\"loading tensorflow datasets ...\")\n",
    "    train_dataset = tf_dataset_from_dataset(train_dataset)\n",
    "    val_dataset = tf_dataset_from_dataset(val_dataset)\n",
    "    test_dataset = tf_dataset_from_dataset(test_dataset)\n",
    "    \n",
    "    print(\"batching and shuffling tensorflow datasets ... \")\n",
    "    train_dataset = shuffle_batch_repeat(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = shuffle_batch_repeat(val_dataset, batch_size=batch_size,)\n",
    "    test_dataset = shuffle_batch_repeat(test_dataset, batch_size=batch_size,)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, hidden_dim=256, dropout=0.2):\n",
    "    \"\"\"Create a classification model for paired embedding inputs\"\"\"\n",
    "    \n",
    "    case_input = tf.keras.layers.Input(shape=(768,), dtype=tf.float32, name=\"case_input\")\n",
    "    citation_input = tf.keras.layers.Input(shape=(768,), dtype=tf.float32, name=\"citation_input\")\n",
    "\n",
    "    case_representation = tf.keras.layers.BatchNormalization()(case_input)\n",
    "    citation_representation = tf.keras.layers.BatchNormalization()(citation_input)\n",
    "\n",
    "    case_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(case_representation)\n",
    "\n",
    "    citation_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(citation_representation)\n",
    "\n",
    "    case_representation = tf.keras.layers.Dropout(dropout,)(case_representation)\n",
    "    citation_representation = tf.keras.layers.Dropout(dropout,)(citation_representation)\n",
    "\n",
    "    case_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(case_representation)\n",
    "    citation_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(citation_representation)\n",
    "\n",
    "    concatenated = tf.keras.layers.Concatenate()([case_representation, citation_representation])\n",
    "\n",
    "    def shared_stack(prev_input):\n",
    "        return tf.keras.models.Sequential(\n",
    "            [tf.keras.layers.BatchNormalization(),\n",
    "             tf.keras.layers.Dense(hidden_dim, activation=\"relu\"),\n",
    "             tf.keras.layers.Dropout(dropout)\n",
    "            ])(prev_input) \n",
    "    concatenated = shared_stack(concatenated)\n",
    "    concatenated = shared_stack(concatenated)\n",
    "    concatenated = shared_stack(concatenated)\n",
    "    output = tf.keras.layers.Dense(1, dtype=tf.float32, activation=\"sigmoid\")(concatenated)\n",
    "    clf_model = tf.keras.models.Model(inputs=[case_input, citation_input], outputs=[output], name=model_name)\n",
    "    loss= tf.keras.losses.BinaryCrossentropy()\n",
    "    acc = tf.keras.metrics.BinaryAccuracy()\n",
    "    clf_model.compile(loss=loss, metrics=[acc],  optimizer=\"adam\")\n",
    "    clf_model.summary()\n",
    "    return clf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"specter_pooled_clf_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "case_input (InputLayer)         [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "citation_input (InputLayer)     [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 768)          3072        case_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 768)          3072        citation_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          196864      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          65792       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 512)          0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 256)          133376      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 256)          66816       sequential[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 256)          66816       sequential_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            257         sequential_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 798,721\n",
      "Trainable params: 793,601\n",
      "Non-trainable params: 5,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clf_model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "             tf.keras.callbacks.ModelCheckpoint(model_checkpoint, save_best_only=True,),\n",
    "             tf.keras.callbacks.CSVLogger(model_log_fname),\n",
    "             tf.keras.callbacks.ReduceLROnPlateau(patience=3, verbose=1)\n",
    "            ]\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing dataset dict. loading from disk ...\n",
      "loading tensorflow datasets ...\n",
      "batching and shuffling tensorflow datasets ... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset, val_dataset, test_dataset = load_clf_datasets(\n",
    "    clf_dataset_dir, data_files, batch_size=128, embeddings_dataset=embeddings_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1000/1000 [==============================] - 913s 907ms/step - loss: 0.5247 - binary_accuracy: 0.7306 - val_loss: 0.4713 - val_binary_accuracy: 0.8013\n",
      "INFO:tensorflow:Assets written to: ../models/model_name/assets\n",
      "Epoch 2/25\n",
      " 148/1000 [===>..........................] - ETA: 11:51 - loss: 0.3860 - binary_accuracy: 0.8290"
     ]
    }
   ],
   "source": [
    "history = clf_model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=1000,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=100,\n",
    "    verbose=1,\n",
    "    epochs=25,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "clf_model.save(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model.evaluate(test_dataset, steps=test_dataset.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf_model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(preds > 0.8).flatten().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.from_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
