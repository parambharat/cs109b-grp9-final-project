{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import datasets\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import normalize\n",
    "import faiss\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA A100-PCIE-40GB, compute capability 8.0\n"
     ]
    }
   ],
   "source": [
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Delcare filepaths\n",
    "\n",
    "#dataset directory\n",
    "base_data_dir = \"../data/subset\"\n",
    "\n",
    "#base dataset\n",
    "data_fname = f\"{base_data_dir}/case_info.json\"\n",
    "\n",
    "#embeddings dataset\n",
    "embeddings_model_name=\"allenai/specter\"\n",
    "embedding_type = \"specter_cls\"\n",
    "embedding_dataset_dir = f\"{base_data_dir}/{embedding_type}_embeddings_dataset\"\n",
    "\n",
    "#classification dataset\n",
    "clf_dataset_dir = f\"{base_data_dir}/{embedding_type}_clf_dataset\"\n",
    "data_files = {\n",
    "    \"train\": f\"{base_data_dir}/train_map.csv\", \n",
    "    \"validation\": f\"{base_data_dir}/val_map.csv\", \n",
    "    \"test\": f\"{base_data_dir}/test_map.csv\",}\n",
    "\n",
    "#model\n",
    "models_dir = \"../models\"\n",
    "model_name = \"specter_cls_clf_model\"\n",
    "model_checkpoint = f\"{models_dir}/{model_name}\"\n",
    "model_log_fname = f\"{models_dir}/logs/{model_name}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(examples):\n",
    "    \"\"\"Create text features from head matter and opinion text for a batch of examples\"\"\"\n",
    "    \n",
    "    batch = [item[0] + \"\\n\" + item[1] for item in zip(examples['head_matter'], examples['opinion_text'])]\n",
    "    return {\"text\": batch}\n",
    "\n",
    "def load_embeddings(examples, tokenizer, model, embedding_type):\n",
    "    \"\"\"Tokenize and load embeddings from given huggingface pretrained model\"\"\"\n",
    "    \n",
    "    tokenized = tokenizer(examples[\"text\"],\n",
    "                          return_tensors=\"tf\",\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          max_length=512\n",
    "                         )\n",
    "    if embedding_type == \"pooled\":\n",
    "        embeddings = {'embeddings': normalize(model(**tokenized)[1].numpy())}\n",
    "    else:\n",
    "        embeddings = {'embeddings': normalize(model(**tokenized)[0][:,0, :].numpy())}\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def load_embeddings_dataset(\n",
    "    dataset_dir, embedding_type, embedding_model=\"allenai/specter\",\n",
    "    num_proc=15, batch_size=256, faiss_device=0, keep_in_memory=False):\n",
    "    \"\"\"Load embeddings dataset and create faiss index on embeddings column\"\"\"\n",
    "    \n",
    "    if os.path.isfile(f\"{dataset_dir}/state.json\"):\n",
    "        print(\"Found existing embeddings. loading from disk ...\")\n",
    "        dataset = datasets.Dataset.load_from_disk(dataset_dir, keep_in_memory=keep_in_memory)\n",
    "        if os.path.isfile(f\"{dataset_dir}/embeddings.faiss\"):\n",
    "            print(\"Found existing fiass index. loading from disk ...\")\n",
    "            dataset.load_faiss_index(\"embeddings\", f\"{dataset_dir}/embeddings.faiss\")\n",
    "        else:\n",
    "            print(\"No fiass index found. creating and saving new index to disk ...\")\n",
    "            dataset.add_faiss_index(column=\"embeddings\", device=faiss_device, metric_type=faiss.METRIC_INNER_PRODUCT)\n",
    "            dataset.save_faiss_index(\"embeddings\", f\"{dataset_dir}/embeddings.faiss\")\n",
    "    else:\n",
    "        print(\"No existing embeddings found. Creating and saving to disk ...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
    "        model = TFAutoModel.from_pretrained(embedding_model, from_pt=True)\n",
    "        \n",
    "        print(\"Loading dataset and text column ...\")\n",
    "        dataset = datasets.load_dataset(\"json\", data_files=data_fname, split=datasets.splits.Split(\"train\"))\n",
    "        exclude_columns = [\"jurisdiction_id\",\"court_id\",\"decision_date\", \"head_matter\",\"opinion_text\",\"citation_ids\"]\n",
    "        dataset = dataset.map(load_text, batched=True, num_proc=num_proc, remove_columns=exclude_columns)\n",
    "        print(\"Loading embeddings ...\")\n",
    "        embedder = partial(load_embeddings, tokenizer=tokenizer, model=model, embedding_type=embedding_type.split(\"_\")[-1])\n",
    "        dataset = dataset.map(embedder, batched=True, batch_size=batch_size)\n",
    "        print(f\"Saving Dataset to disk at {dataset_dir}\")\n",
    "        dataset.save_to_disk(dataset_dir)\n",
    "        print(\"Creating new fiass index and saving to disk ...\")\n",
    "        dataset.add_faiss_index(column=\"embeddings\", device=faiss_device, metric_type=faiss.METRIC_INNER_PRODUCT)\n",
    "        dataset.save_faiss_index(\"embeddings\", f\"{dataset_dir}/embeddings.faiss\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing embeddings. loading from disk ...\n",
      "Found existing fiass index. loading from disk ...\n"
     ]
    }
   ],
   "source": [
    "embeddings_dataset = load_embeddings_dataset(\n",
    "    embedding_dataset_dir,\n",
    "    embedding_type,\n",
    "    embedding_model=embeddings_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clf_embeddings(examples):\n",
    "    \"\"\"load embeddings into classification dataset.\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"case_embedding\": embeddings_dataset[examples[\"id\"]][\"embeddings\"],\n",
    "        \"citation_embedding\": embeddings_dataset[examples[\"citation\"]][\"embeddings\"],\n",
    "        }\n",
    "\n",
    "\n",
    "def generator_from_dataset(dataset):\n",
    "    \"\"\"Create a generator from a huggignface dataset.\"\"\"\n",
    "    \n",
    "    def _gen():\n",
    "        for item in dataset:\n",
    "            features = (item[\"case_embedding\"], item[\"citation_embedding\"])\n",
    "            yield features, item['label']\n",
    "    return _gen\n",
    "\n",
    "\n",
    "def tf_dataset_from_dataset(dataset):\n",
    "    \"\"\"Create a tensorflow dataset from a huggingface dataset using a generator.\"\"\"\n",
    "    \n",
    "    dataset_generator = generator_from_dataset(dataset)\n",
    "    tfdataset = tf.data.Dataset.from_generator(\n",
    "        dataset_generator,\n",
    "        output_signature=(\n",
    "         (tf.TensorSpec(shape=(768,), dtype=tf.float32),\n",
    "          tf.TensorSpec(shape=(768,), dtype=tf.float32),\n",
    "         ),\n",
    "         tf.TensorSpec(shape=(None), dtype=tf.int32))\n",
    "    )\n",
    "    tfdataset = tfdataset.apply(tf.data.experimental.assert_cardinality(len(dataset)))\n",
    "    return tfdataset\n",
    "\n",
    "def shuffle_batch_repeat(dataset, batch_size=64, shuffle=False):\n",
    "    \"\"\"Batch shuffle and repeat a tensorflow dataset infinitely.\"\"\"\n",
    "    \n",
    "    dataset = dataset.repeat()\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size*4)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_clf_datasets(\n",
    "    dataset_dir, data_files, batch_size=64,\n",
    "    embeddings_dataset=None, num_proc=15):\n",
    "    \"\"\"Create tensorflow dataset with pairs for embeddings and a label for classification.\"\"\"\n",
    "    \n",
    "    if os.path.isfile(f\"{dataset_dir}/dataset_dict.json\"):\n",
    "        print(\"Found existing dataset dict. loading from disk ...\")\n",
    "        dataset = datasets.DatasetDict.load_from_disk(dataset_dir, keep_in_memory=True)\n",
    "        dataset.set_format(type='tensorflow', columns=['case_embedding', 'citation_embedding', 'label'])\n",
    "        \n",
    "        train_dataset = dataset['train']\n",
    "        val_dataset = dataset['validation']\n",
    "        test_dataset = dataset['test']\n",
    "        \n",
    "    elif embeddings_dataset is not None:\n",
    "        print(\"Found data files and embeddings dataset. Creating new clf dataset and saving to disk ...\")\n",
    "        dataset = datasets.load_dataset(\"csv\", data_files=data_files)\n",
    "        print(\"Mapping embeddings to clf dataset ...\")        \n",
    "        dataset = dataset.map(load_clf_embeddings, batched=True, num_proc=num_proc)\n",
    "        print(f\"Saving Dataset to disk at {dataset_dir}\")\n",
    "        dataset.save_to_disk(dataset_dir)\n",
    "        dataset.set_format(type='tensorflow', columns=['case_embedding', 'citation_embedding', 'label'])\n",
    "        \n",
    "        train_dataset = dataset['train']\n",
    "        val_dataset = dataset['validation']\n",
    "        test_dataset = dataset['test']\n",
    "\n",
    "    print(\"loading tensorflow datasets ...\")\n",
    "    train_dataset = tf_dataset_from_dataset(train_dataset)\n",
    "    val_dataset = tf_dataset_from_dataset(val_dataset)\n",
    "    test_dataset = tf_dataset_from_dataset(test_dataset)\n",
    "    \n",
    "    print(\"batching and shuffling tensorflow datasets ... \")\n",
    "    train_dataset = shuffle_batch_repeat(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = shuffle_batch_repeat(val_dataset, batch_size=batch_size,)\n",
    "    test_dataset = shuffle_batch_repeat(test_dataset, batch_size=batch_size,)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, hidden_dim=256, dropout=0.2):\n",
    "    \"\"\"Create a classification model for paired embedding inputs\"\"\"\n",
    "    \n",
    "    case_input = tf.keras.layers.Input(shape=(768,), dtype=tf.float32, name=\"case_input\")\n",
    "    citation_input = tf.keras.layers.Input(shape=(768,), dtype=tf.float32, name=\"citation_input\")\n",
    "\n",
    "    case_representation = tf.keras.layers.BatchNormalization()(case_input)\n",
    "    citation_representation = tf.keras.layers.BatchNormalization()(citation_input)\n",
    "\n",
    "    case_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(case_representation)\n",
    "\n",
    "    citation_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(citation_representation)\n",
    "\n",
    "    case_representation = tf.keras.layers.Dropout(dropout,)(case_representation)\n",
    "    citation_representation = tf.keras.layers.Dropout(dropout,)(citation_representation)\n",
    "\n",
    "    case_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(case_representation)\n",
    "    citation_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(citation_representation)\n",
    "    \n",
    "    sims = tf.keras.layers.Dot(axes=1, normalize=True)([case_representation, citation_representation])\n",
    "    concatenated = tf.keras.layers.Concatenate()([case_representation, citation_representation])\n",
    "\n",
    "    def shared_stack(prev_input):\n",
    "        return tf.keras.models.Sequential(\n",
    "            [tf.keras.layers.BatchNormalization(),\n",
    "             tf.keras.layers.Dense(hidden_dim, activation=\"relu\"),\n",
    "             tf.keras.layers.Dropout(dropout)\n",
    "            ])(prev_input) \n",
    "    concatenated = shared_stack(concatenated)\n",
    "    concatenated = shared_stack(concatenated)\n",
    "    concatenated = shared_stack(concatenated)\n",
    "    output = tf.keras.layers.Dense(1, dtype=tf.float32, activation=\"sigmoid\")(concatenated)\n",
    "    output = tf.keras.layers.Average()([sims,output])\n",
    "    clf_model = tf.keras.models.Model(inputs=[case_input, citation_input], outputs=[output], name=model_name)\n",
    "    loss= tf.keras.losses.BinaryCrossentropy()\n",
    "    acc = tf.keras.metrics.BinaryAccuracy()\n",
    "    clf_model.compile(loss=loss, metrics=[acc],  optimizer=\"adam\")\n",
    "    clf_model.summary()\n",
    "    return clf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"specter_cls_clf_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "case_input (InputLayer)         [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "citation_input (InputLayer)     [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 768)          3072        case_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 768)          3072        citation_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          196864      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          65792       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 512)          0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 256)          133376      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 256)          66816       sequential[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 256)          66816       sequential_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1)            0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            257         sequential_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "average (Average)               (None, 1)            0           dot[0][0]                        \n",
      "                                                                 dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 798,721\n",
      "Trainable params: 793,601\n",
      "Non-trainable params: 5,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clf_model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "             tf.keras.callbacks.ModelCheckpoint(model_checkpoint, save_best_only=True,),\n",
    "             tf.keras.callbacks.CSVLogger(model_log_fname),\n",
    "             tf.keras.callbacks.ReduceLROnPlateau(patience=3, verbose=1)\n",
    "            ]\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing dataset dict. loading from disk ...\n",
      "loading tensorflow datasets ...\n",
      "batching and shuffling tensorflow datasets ... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset, val_dataset, test_dataset = load_clf_datasets(\n",
    "    clf_dataset_dir, data_files, batch_size=128, embeddings_dataset=embeddings_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1000/1000 [==============================] - 879s 874ms/step - loss: 0.5067 - binary_accuracy: 0.7483 - val_loss: 0.4466 - val_binary_accuracy: 0.8101\n",
      "INFO:tensorflow:Assets written to: ../models/specter_cls_clf_model/assets\n",
      "Epoch 2/15\n",
      "1000/1000 [==============================] - 876s 877ms/step - loss: 0.3812 - binary_accuracy: 0.8431 - val_loss: 0.4279 - val_binary_accuracy: 0.8267\n",
      "INFO:tensorflow:Assets written to: ../models/specter_cls_clf_model/assets\n",
      "Epoch 3/15\n",
      "1000/1000 [==============================] - 897s 898ms/step - loss: 0.3571 - binary_accuracy: 0.8581 - val_loss: 0.4296 - val_binary_accuracy: 0.8224\n",
      "Epoch 4/15\n",
      "1000/1000 [==============================] - 898s 899ms/step - loss: 0.3430 - binary_accuracy: 0.8671 - val_loss: 0.3920 - val_binary_accuracy: 0.8480\n",
      "INFO:tensorflow:Assets written to: ../models/specter_cls_clf_model/assets\n",
      "Epoch 5/15\n",
      "1000/1000 [==============================] - 911s 912ms/step - loss: 0.3285 - binary_accuracy: 0.8758 - val_loss: 0.3909 - val_binary_accuracy: 0.8447\n",
      "INFO:tensorflow:Assets written to: ../models/specter_cls_clf_model/assets\n",
      "Epoch 6/15\n",
      "1000/1000 [==============================] - 949s 949ms/step - loss: 0.3220 - binary_accuracy: 0.8814 - val_loss: 0.3758 - val_binary_accuracy: 0.8547\n",
      "INFO:tensorflow:Assets written to: ../models/specter_cls_clf_model/assets\n",
      "Epoch 7/15\n",
      "1000/1000 [==============================] - 985s 986ms/step - loss: 0.3136 - binary_accuracy: 0.8860 - val_loss: 0.3589 - val_binary_accuracy: 0.8685\n",
      "INFO:tensorflow:Assets written to: ../models/specter_cls_clf_model/assets\n",
      "Epoch 8/15\n",
      "1000/1000 [==============================] - 940s 941ms/step - loss: 0.3131 - binary_accuracy: 0.8853 - val_loss: 0.3557 - val_binary_accuracy: 0.8712\n",
      "INFO:tensorflow:Assets written to: ../models/specter_cls_clf_model/assets\n",
      "Epoch 9/15\n",
      "1000/1000 [==============================] - 948s 949ms/step - loss: 0.3068 - binary_accuracy: 0.8854 - val_loss: 0.3387 - val_binary_accuracy: 0.8841\n",
      "INFO:tensorflow:Assets written to: ../models/specter_cls_clf_model/assets\n",
      "Epoch 10/15\n",
      "1000/1000 [==============================] - 928s 929ms/step - loss: 0.3042 - binary_accuracy: 0.8928 - val_loss: 0.3929 - val_binary_accuracy: 0.8443\n",
      "Epoch 11/15\n",
      "1000/1000 [==============================] - 954s 955ms/step - loss: 0.2949 - binary_accuracy: 0.9000 - val_loss: 0.3948 - val_binary_accuracy: 0.8439\n",
      "Epoch 12/15\n",
      "1000/1000 [==============================] - 952s 953ms/step - loss: 0.2989 - binary_accuracy: 0.8917 - val_loss: 0.3646 - val_binary_accuracy: 0.8574\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 13/15\n",
      "1000/1000 [==============================] - 952s 953ms/step - loss: 0.3003 - binary_accuracy: 0.8823 - val_loss: 0.3579 - val_binary_accuracy: 0.8616\n",
      "Epoch 14/15\n",
      "1000/1000 [==============================] - 950s 951ms/step - loss: 0.2982 - binary_accuracy: 0.8849 - val_loss: 0.3561 - val_binary_accuracy: 0.8633\n",
      "INFO:tensorflow:Assets written to: ../models/specter_cls_clf_model/assets\n"
     ]
    }
   ],
   "source": [
    "history = clf_model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=1000,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=100,\n",
    "    verbose=1,\n",
    "    epochs=15,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "clf_model.save(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = load_embeddings_dataset(\n",
    "    embedding_dataset_dir,\n",
    "    embedding_type,\n",
    "    embedding_model=embeddings_model_name, keep_in_memory=True)\n",
    "\n",
    "database.set_format(\"numpy\", columns=[\"embeddings\", \"id\"], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/subset/train_map.csv\",)\n",
    "val_df = pd.read_csv(\"../data/subset/val_map.csv\", )\n",
    "test_df = pd.read_csv(\"../data/subset/test_map.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.groupby(\"id\").agg({\"citation\": list})\n",
    "val_df = val_df.groupby(\"id\").agg({\"citation\": list})\n",
    "test_df = test_df.groupby(\"id\").agg({\"citation\": list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k_preds(df, k=50, key=None):\n",
    "    query_embeddings = database[df.index.tolist()]['embeddings']\n",
    "    scores, samples = database.get_nearest_examples_batch('embeddings', query_embeddings, k=k)\n",
    "    if key:\n",
    "        preds = [sample[key] for sample in samples]\n",
    "    else:\n",
    "        preds = samples\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = retrieve_top_k_preds(val_df, key=\"id\")\n",
    "test_preds = retrieve_top_k_preds(test_df, key=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    #https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "    Computes the average precision at k.\n",
    "    \"\"\"\n",
    "    if len(predicted)>k:\n",
    "        predicted = predicted[:k]\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i+1.0)\n",
    "\n",
    "    if not actual:\n",
    "        return 0.0\n",
    "\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=10):\n",
    "    \"\"\"\n",
    "    #https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "    Computes the mean average precision at k.\n",
    "    \"\"\"\n",
    "    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2cite=json.load(open(\"../data/subset/idx2cite.json\"))\n",
    "idx2cite = {int(k):int(v) for k,v in idx2cite.items()}\n",
    "\n",
    "tocite = lambda x: [idx2cite.get(y) for y in x]\n",
    "val_df.loc[: , \"cite_id\"] = val_df[\"citation\"].map(tocite)\n",
    "test_df.loc[: , \"cite_id\"] = test_df[\"citation\"].map(tocite)\n",
    "\n",
    "def load_map(df, preds):\n",
    "    scores = []\n",
    "    for k in tqdm.tnrange(5, 30, 5):\n",
    "        scores.append(mapk(df.cite_id.tolist(), preds, k))\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores = load_map(val_df, val_preds)\n",
    "test_scores = load_map(test_df, val_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(val_scores, test_scores, col_names = [\"val_map\", \"test_map\"]):\n",
    "    plot_df = pd.DataFrame(list(zip(val_scores, test_scores)), columns=col_names, index=range(5, 30, 5))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    sns.lineplot(data=plot_df, markers=True)\n",
    "    plt.title(\"mean average precision(MAP) at various k's recommended samples\")\n",
    "    plt.xlabel(\"top-k\")\n",
    "    sns.despine()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(val_scores, test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cite2idx=json.load(open(\"../data/subset/cite2idx.json\"))\n",
    "cite2idx = {int(k):int(v) for k,v in cite2idx.items()}\n",
    "\n",
    "toidx = lambda x: [cite2idx.get(y) for y in x]\n",
    "\n",
    "\n",
    "def get_sorted_preds(model, df, preds):\n",
    "    filtered_preds = []\n",
    "    preds = list(map(toidx, preds))\n",
    "    for idx, pred in tqdm.tqdm_notebook(zip(df.index, preds), total=len(df)):\n",
    "        num_results = len(pred)\n",
    "        result_embeddings = database[pred]['embeddings']\n",
    "        query_embeddings = np.array([database[idx]['embeddings']]*num_results)\n",
    "        preds = model.predict((query_embeddings, result_embeddings), batch_size=num_results)\n",
    "        preds = preds.flatten()\n",
    "        filtered_preds.append(np.array(pred)[np.argsort(preds)])\n",
    "    return filtered_preds\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model = tf.keras.models.load_model(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_preds = get_top_filtered_k(clf_model, val_df.head(100), val_preds[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in tqdm.tnrange(1, 50):\n",
    "    scores.append(mapk(val_df.head(100).citation, filtered_preds, k=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision`_recall_k(y_true, y_pred, k=10):\n",
    "    precisions =[]\n",
    "    recalls = []\n",
    "    for y_t, y_p in zip(y_true, y_pred):\n",
    "        y_p = set(y_p[:k])\n",
    "        relevant_retrieved = set(y_t).intersection(y_p)\n",
    "        if relevant_retrieved:\n",
    "            precision = len(relevant_retrieved)/len(y_p)\n",
    "            recall = len(relevant_retrieved)/len(y_t)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "    return np.mean(precisions), np.mean(recalls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
