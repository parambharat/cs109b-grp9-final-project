{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import datasets\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import normalize\n",
    "import faiss\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Delcare filepaths\n",
    "\n",
    "#dataset directory\n",
    "base_data_dir = \"../data/subset\"\n",
    "\n",
    "#base dataset\n",
    "data_fname = f\"{base_data_dir}/case_info.json\"\n",
    "\n",
    "#embeddings dataset\n",
    "embeddings_model_name=\"allenai/specter\"\n",
    "embedding_type = \"specter_pooled\"\n",
    "embedding_dataset_dir = f\"{base_data_dir}/{embedding_type}_embeddings_dataset\"\n",
    "\n",
    "#classification dataset\n",
    "clf_dataset_dir = f\"{base_data_dir}/{embedding_type}_clf_dataset\"\n",
    "data_files = {\n",
    "    \"train\": f\"{base_data_dir}/train_map.csv\", \n",
    "    \"validation\": f\"{base_data_dir}/val_map.csv\", \n",
    "    \"test\": f\"{base_data_dir}/test_map.csv\",}\n",
    "\n",
    "#model\n",
    "BATCH_SIZE= 128\n",
    "STEP_PER_EPOCH = 1000\n",
    "VAL_STEPS = 100\n",
    "\n",
    "models_dir = \"../models\"\n",
    "model_name = \"specter_pooled_clf_model\"\n",
    "model_checkpoint = f\"{models_dir}/{model_name}\"\n",
    "model_log_fname = f\"{models_dir}/logs/{model_name}.csv\"\n",
    "model_png = f\"{models_dir}/logs/{model_name}.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(examples):\n",
    "    \"\"\"Create text features from head matter and opinion text for a batch of examples\"\"\"\n",
    "    \n",
    "    batch = [item[0] + \"\\n\" + item[1] for item in zip(examples['head_matter'], examples['opinion_text'])]\n",
    "    return {\"text\": batch}\n",
    "\n",
    "def load_embeddings(examples, tokenizer, model, embedding_type):\n",
    "    \"\"\"Tokenize and load embeddings from given huggingface pretrained model\"\"\"\n",
    "    \n",
    "    tokenized = tokenizer(examples[\"text\"],\n",
    "                          return_tensors=\"tf\",\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          max_length=512\n",
    "                         )\n",
    "    if embedding_type == \"pooled\":\n",
    "        embeddings = {'embeddings': normalize(model(**tokenized)[1].numpy())}\n",
    "    else:\n",
    "        embeddings = {'embeddings': normalize(model(**tokenized)[0][:,0, :].numpy())}\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def load_embeddings_dataset(\n",
    "    dataset_dir, data_fname, embedding_type, embedding_model=\"allenai/specter\",\n",
    "    num_proc=15, batch_size=256, faiss_device=0, keep_in_memory=False):\n",
    "    \"\"\"Load embeddings dataset and create faiss index on embeddings column\"\"\"\n",
    "    \n",
    "    if os.path.isfile(f\"{dataset_dir}/state.json\"):\n",
    "        print(\"Found existing embeddings. loading from disk ...\")\n",
    "        dataset = datasets.Dataset.load_from_disk(dataset_dir, keep_in_memory=keep_in_memory)\n",
    "        if os.path.isfile(f\"{dataset_dir}/embeddings.faiss\"):\n",
    "            print(\"Found existing fiass index. loading from disk ...\")\n",
    "            dataset.load_faiss_index(\"embeddings\", f\"{dataset_dir}/embeddings.faiss\")\n",
    "        else:\n",
    "            print(\"No fiass index found. creating and saving new index to disk ...\")\n",
    "            dataset.add_faiss_index(column=\"embeddings\", device=faiss_device, metric_type=faiss.METRIC_INNER_PRODUCT)\n",
    "            dataset.save_faiss_index(\"embeddings\", f\"{dataset_dir}/embeddings.faiss\")\n",
    "    else:\n",
    "        print(\"No existing embeddings found. Creating and saving to disk ...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
    "        model = TFAutoModel.from_pretrained(embedding_model, from_pt=True)\n",
    "        \n",
    "        print(\"Loading dataset and text column ...\")\n",
    "        dataset = datasets.load_dataset(\"json\", data_files=data_fname, split=datasets.splits.Split(\"train\"))\n",
    "        exclude_columns = [\"jurisdiction_id\",\"court_id\",\"decision_date\", \"head_matter\",\"opinion_text\",\"citation_ids\"]\n",
    "        dataset = dataset.map(load_text, batched=True, num_proc=num_proc, remove_columns=exclude_columns)\n",
    "        print(\"Loading embeddings ...\")\n",
    "        embedder = partial(load_embeddings, tokenizer=tokenizer, model=model, embedding_type=embedding_type.split(\"_\")[-1])\n",
    "        dataset = dataset.map(embedder, batched=True, batch_size=batch_size)\n",
    "        print(f\"Saving Dataset to disk at {dataset_dir}\")\n",
    "        dataset.save_to_disk(dataset_dir)\n",
    "        print(\"Creating new fiass index and saving to disk ...\")\n",
    "        dataset.add_faiss_index(column=\"embeddings\", device=faiss_device, metric_type=faiss.METRIC_INNER_PRODUCT)\n",
    "        dataset.save_faiss_index(\"embeddings\", f\"{dataset_dir}/embeddings.faiss\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dataset = load_embeddings_dataset(\n",
    "    embedding_dataset_dir,\n",
    "    data_fname,\n",
    "    embedding_type,\n",
    "    embedding_model=embeddings_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clf_embeddings(examples):\n",
    "    \"\"\"load embeddings into classification dataset.\"\"\"\n",
    "    \n",
    "    return {\n",
    "        \"case_embedding\": embeddings_dataset[examples[\"idx\"]][\"embeddings\"],\n",
    "        \"citation_embedding\": embeddings_dataset[examples[\"citation_idx\"]][\"embeddings\"],\n",
    "        }\n",
    "\n",
    "\n",
    "def generator_from_dataset(dataset):\n",
    "    \"\"\"Create a generator from a huggignface dataset.\"\"\"\n",
    "    \n",
    "    def _gen():\n",
    "        for item in dataset:\n",
    "            features = (item[\"case_embedding\"], item[\"citation_embedding\"])\n",
    "            yield features, item['label']\n",
    "    return _gen\n",
    "\n",
    "\n",
    "def tf_dataset_from_dataset(dataset):\n",
    "    \"\"\"Create a tensorflow dataset from a huggingface dataset using a generator.\"\"\"\n",
    "    \n",
    "    dataset_generator = generator_from_dataset(dataset)\n",
    "    tfdataset = tf.data.Dataset.from_generator(\n",
    "        dataset_generator,\n",
    "        output_signature=(\n",
    "         (tf.TensorSpec(shape=(768,), dtype=tf.float32),\n",
    "          tf.TensorSpec(shape=(768,), dtype=tf.float32),\n",
    "         ),\n",
    "         tf.TensorSpec(shape=(None), dtype=tf.int32))\n",
    "    )\n",
    "    tfdataset = tfdataset.apply(tf.data.experimental.assert_cardinality(len(dataset)))\n",
    "    return tfdataset\n",
    "\n",
    "def shuffle_batch_repeat(dataset, batch_size=64, shuffle=False):\n",
    "    \"\"\"Batch shuffle and repeat a tensorflow dataset infinitely.\"\"\"\n",
    "    \n",
    "    dataset = dataset.repeat()\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size*4)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_clf_datasets(\n",
    "    dataset_dir, data_files, batch_size=64,\n",
    "    embeddings_dataset=None, num_proc=15):\n",
    "    \"\"\"Create tensorflow dataset with pairs for embeddings and a label for classification.\"\"\"\n",
    "    \n",
    "    if os.path.isfile(f\"{dataset_dir}/dataset_dict.json\"):\n",
    "        print(\"Found existing dataset dict. loading from disk ...\")\n",
    "        dataset = datasets.DatasetDict.load_from_disk(dataset_dir, keep_in_memory=True)\n",
    "        dataset.set_format(type='tensorflow', columns=['case_embedding', 'citation_embedding', 'label'])\n",
    "        \n",
    "        train_dataset = dataset['train']\n",
    "        val_dataset = dataset['validation']\n",
    "        test_dataset = dataset['test']\n",
    "        \n",
    "    elif embeddings_dataset is not None:\n",
    "        print(\"Found data files and embeddings dataset. Creating new clf dataset and saving to disk ...\")\n",
    "        dataset = datasets.load_dataset(\"csv\", data_files=data_files)\n",
    "        print(\"Mapping embeddings to clf dataset ...\")        \n",
    "        dataset = dataset.map(load_clf_embeddings, batched=True, num_proc=num_proc)\n",
    "        print(f\"Saving Dataset to disk at {dataset_dir}\")\n",
    "        dataset.save_to_disk(dataset_dir)\n",
    "        dataset.set_format(type='tensorflow', columns=['case_embedding', 'citation_embedding', 'label'])\n",
    "        \n",
    "        train_dataset = dataset['train']\n",
    "        val_dataset = dataset['validation']\n",
    "        test_dataset = dataset['test']\n",
    "\n",
    "    print(\"loading tensorflow datasets ...\")\n",
    "    train_dataset = tf_dataset_from_dataset(train_dataset)\n",
    "    val_dataset = tf_dataset_from_dataset(val_dataset)\n",
    "    test_dataset = tf_dataset_from_dataset(test_dataset)\n",
    "    \n",
    "    print(\"batching and shuffling tensorflow datasets ... \")\n",
    "    train_dataset = shuffle_batch_repeat(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = shuffle_batch_repeat(val_dataset, batch_size=batch_size,)\n",
    "    test_dataset = shuffle_batch_repeat(test_dataset, batch_size=batch_size,)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = load_clf_datasets(\n",
    "    clf_dataset_dir, data_files, batch_size=BATCH_SIZE, embeddings_dataset=embeddings_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, hidden_dim=256, dropout=0.2):\n",
    "    \"\"\"Create a classification model for paired embedding inputs\"\"\"\n",
    "    \n",
    "    case_input = tf.keras.layers.Input(shape=(768,), dtype=tf.float32, name=\"case_input\")\n",
    "    citation_input = tf.keras.layers.Input(shape=(768,), dtype=tf.float32, name=\"citation_input\")\n",
    "\n",
    "    case_representation = tf.keras.layers.BatchNormalization()(case_input)\n",
    "    citation_representation = tf.keras.layers.BatchNormalization()(citation_input)\n",
    "\n",
    "    case_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(case_representation)\n",
    "\n",
    "    citation_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(citation_representation)\n",
    "\n",
    "    case_representation = tf.keras.layers.Dropout(dropout,)(case_representation)\n",
    "    citation_representation = tf.keras.layers.Dropout(dropout,)(citation_representation)\n",
    "\n",
    "    case_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(case_representation)\n",
    "    citation_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(citation_representation)\n",
    "    \n",
    "    sims = tf.keras.layers.Dot(axes=1, normalize=True)([case_representation, citation_representation])\n",
    "    concatenated = tf.keras.layers.Concatenate()([case_representation, citation_representation])\n",
    "\n",
    "    def shared_stack(prev_input):\n",
    "        return tf.keras.models.Sequential(\n",
    "            [tf.keras.layers.BatchNormalization(),\n",
    "             tf.keras.layers.Dense(hidden_dim, activation=\"relu\"),\n",
    "             tf.keras.layers.Dropout(dropout)\n",
    "            ])(prev_input) \n",
    "    concatenated = shared_stack(concatenated)\n",
    "    concatenated = shared_stack(concatenated)\n",
    "    concatenated = shared_stack(concatenated)\n",
    "    output = tf.keras.layers.Dense(1, dtype=tf.float32, activation=\"sigmoid\")(concatenated)\n",
    "    output = tf.keras.layers.Average()([sims,output])\n",
    "    clf_model = tf.keras.models.Model(inputs=[case_input, citation_input], outputs=[output], name=model_name)\n",
    "    loss= tf.keras.losses.BinaryCrossentropy()\n",
    "    acc = tf.keras.metrics.BinaryAccuracy()\n",
    "    clf_model.compile(loss=loss, metrics=[acc],  optimizer=\"adam\")\n",
    "    clf_model.summary()\n",
    "    return clf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    clf_model,\n",
    "    model_png,\n",
    "    show_shapes=False,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "             tf.keras.callbacks.ModelCheckpoint(model_checkpoint, save_best_only=True,),\n",
    "             tf.keras.callbacks.CSVLogger(model_log_fname),\n",
    "             tf.keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1)\n",
    "            ]\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE= 128\n",
    "STEP_PER_EPOCH = 1000\n",
    "VAL_STEPS = 100\n",
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = clf_model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=STEP_PER_EPOCH,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=VAL_STEPS,\n",
    "    verbose=1,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "clf_model.save(model_checkpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
