{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import datasets\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import normalize\n",
    "import faiss\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "from tensorflow.keras import mixed_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA A100-PCIE-40GB, compute capability 8.0\n"
     ]
    }
   ],
   "source": [
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_dir = \"../data/subset\"\n",
    "\n",
    "data_fname = f\"{base_data_dir}/case_info.json\"\n",
    "embeddings_model_name=\"allenai/specter\"\n",
    "embedding_type = \"specter_pooled\"\n",
    "embedding_dataset_dir = f\"{base_data_dir}/{embedding_type}_embeddings_dataset\"\n",
    "clf_dataset_dir = f\"{base_data_dir}/{embedding_type}_clf_dataset\"\n",
    "\n",
    "models_dir = \"../models\"\n",
    "model_name = \"specter_pooled_clf_model\"\n",
    "model_checkpoint = f\"{models_dir}/model_name\"\n",
    "model_log_fname = f\"{models_dir}/logs/{model_name}.csv\"\n",
    "\n",
    "\n",
    "data_files = {\n",
    "    \"train\": f\"{base_data_dir}/train_map.csv\", \n",
    "    \"validation\": f\"{base_data_dir}/val_map.csv\", \n",
    "    \"test\": f\"{base_data_dir}/test_map.csv\",}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(examples):\n",
    "    return {\"text\": [item[0] + \"\\n\" + item[1] for item in zip(examples['head_matter'], examples['opinion_text'])]}\n",
    "\n",
    "def load_embeddings(examples, tokenizer, model, embedding_type):\n",
    "    tokenized = tokenizer(examples[\"text\"],\n",
    "                          return_tensors=\"tf\",\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          max_length=512\n",
    "                         )\n",
    "    if embedding_type == \"pooled\":\n",
    "        embeddings = {'embeddings': normalize(model(**tokenized)[1].numpy())}\n",
    "    else:\n",
    "        embeddings = {'embeddings': normalize(model(**tokenized)[0][:,0:].numpy())}\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def load_embeddings_dataset(dataset_dir, embedding_type, embedding_model=\"allenai/specter\", num_proc=15, batch_size=256, faiss_device=0):\n",
    "    if os.path.isfile(f\"{dataset_dir}/state.json\"):\n",
    "        print(\"Found existing embeddings. loading from disk ...\")\n",
    "        dataset = datasets.Dataset.load_from_disk(dataset_dir)\n",
    "        if os.path.isfile(f\"{dataset_dir}/embeddings.faiss\"):\n",
    "            print(\"Found existing fiass index. loading from disk ...\")\n",
    "            dataset.load_faiss_index(\"embeddings\", f\"{dataset_dir}/embeddings.faiss\")\n",
    "        else:\n",
    "            print(\"No fiass index found. creating and saving new index to disk ...\")\n",
    "            dataset.add_faiss_index(column=\"embeddings\", device=faiss_device, metric_type=faiss.METRIC_INNER_PRODUCT)\n",
    "            dataset.save_faiss_index(\"embeddings\", f\"{dataset_dir}/embeddings.faiss\")\n",
    "    else:\n",
    "        print(\"No existing embeddings found. Creating and saving to disk ...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
    "        model = TFAutoModel.from_pretrained(embedding_model, from_pt=True)\n",
    "        \n",
    "        print(\"Loading dataset and text column ...\")\n",
    "        dataset = datasets.load_dataset(\"json\", data_files=data_fname, split=datasets.splits.Split(\"train\"))\n",
    "        exclude_columns = [\"jurisdiction_id\",\"court_id\",\"decision_date\", \"head_matter\",\"opinion_text\",\"citation_ids\"]\n",
    "        dataset = dataset.map(load_text, batched=True, num_proc=num_proc, remove_columns=exclude_columns)\n",
    "        print(\"Loading embeddings ...\")\n",
    "        embedder = partial(load_embeddings, tokenizer=tokenzier, model=model, embedding_type=embedding_type.split(\"_\")[-1])\n",
    "        dataset = dataset.map(embedder, batched=True, batch_size=batch_size)\n",
    "        print(f\"Saving Dataset to disk at {dataset_dir}\")\n",
    "        dataset.save_to_disk(dataset_dir)\n",
    "        print(\"Creating new fiass index and saving to disk ...\")\n",
    "        dataset.add_faiss_index(column=\"embeddings\", device=faiss_device, metric_type=faiss.METRIC_INNER_PRODUCT)\n",
    "        dataset.save_faiss_index(\"embeddings\", f\"{dataset_dir}/embeddings.faiss\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing embeddings. loading from disk ...\n",
      "Found existing fiass index. loading from disk ...\n"
     ]
    }
   ],
   "source": [
    "embeddings_dataset = load_embeddings_dataset(\n",
    "    embedding_dataset_dir,\n",
    "    embedding_type,\n",
    "    embedding_model=embeddings_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clf_embeddings(examples):\n",
    "    return {\n",
    "        \"case_embedding\": embeddings_dataset[examples[\"id\"]][\"embeddings\"],\n",
    "        \"citation_embedding\": embeddings_dataset[examples[\"citation\"]][\"embeddings\"],\n",
    "        }\n",
    "\n",
    "\n",
    "def generator_from_dataset(dataset):\n",
    "    def _gen():\n",
    "        for item in dataset:\n",
    "            features = (item[\"case_embedding\"], item[\"citation_embedding\"])\n",
    "            yield features, item['label']\n",
    "    return _gen\n",
    "\n",
    "\n",
    "def tf_dataset_from_dataset(dataset):\n",
    "    dataset_generator = generator_from_dataset(dataset)\n",
    "    tfdataset = tf.data.Dataset.from_generator(\n",
    "        dataset_generator,\n",
    "        output_signature=(\n",
    "         (tf.TensorSpec(shape=(768,), dtype=tf.float32),\n",
    "          tf.TensorSpec(shape=(768,), dtype=tf.float32),\n",
    "         ),\n",
    "         tf.TensorSpec(shape=(None), dtype=tf.int32))\n",
    "    )\n",
    "    tfdataset = tfdataset.apply(tf.data.experimental.assert_cardinality(len(dataset)))\n",
    "    return tfdataset\n",
    "\n",
    "def shuffle_batch_repeat(dataset, batch_size=64, shuffle=False):\n",
    "    dataset = dataset.repeat()\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size*4)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def load_clf_datasets(dataset_dir, data_files, batch_size=64, embeddings_dataset=None, num_proc=15):\n",
    "    if os.path.isfile(f\"{dataset_dir}/dataset_dict.json\"):\n",
    "        print(\"Found existing dataset dict. loading from disk ...\")\n",
    "        dataset = datasets.DatasetDict.load_from_disk(dataset_dir, keep_in_memory=True)\n",
    "        dataset.set_format(type='tensorflow', columns=['case_embedding', 'citation_embedding', 'label'])\n",
    "        \n",
    "        train_dataset = dataset['train']\n",
    "        val_dataset = dataset['validation']\n",
    "        test_dataset = dataset['test']\n",
    "        \n",
    "    elif embeddings_dataset is not None:\n",
    "        print(\"Found data files and embeddings dataset. Creating new clf dataset and saving to disk ...\")\n",
    "        dataset = datasets.load_dataset(\"csv\", data_files=data_files)\n",
    "        print(\"Mapping embeddings to clf dataset ...\")        \n",
    "        dataset = dataset.map(load_clf_embeddings, batched=True, num_proc=num_proc)\n",
    "        print(f\"Saving Dataset to disk at {dataset_dir}\")\n",
    "        dataset.save_to_disk(dataset_dir)\n",
    "        dataset.set_format(type='tensorflow', columns=['case_embedding', 'citation_embedding', 'label'])\n",
    "        \n",
    "        train_dataset = dataset['train']\n",
    "        val_dataset = dataset['validation']\n",
    "        test_dataset = dataset['test']\n",
    "\n",
    "    print(\"loading tensorflow datasets ...\")\n",
    "    train_dataset = tf_dataset_from_dataset(train_dataset)\n",
    "    val_dataset = tf_dataset_from_dataset(val_dataset)\n",
    "    test_dataset = tf_dataset_from_dataset(test_dataset)\n",
    "    \n",
    "    print(\"batching and shuffling tensorflow datasets ... \")\n",
    "    train_dataset = shuffle_batch_repeat(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataset = shuffle_batch_repeat(val_dataset, batch_size=batch_size,)\n",
    "    test_dataset = shuffle_batch_repeat(test_dataset, batch_size=batch_size,)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, hidden_dim=256, dropout=0.2):\n",
    "    case_input = tf.keras.layers.Input(shape=(768,), dtype=tf.float32, name=\"case_input\")\n",
    "    citation_input = tf.keras.layers.Input(shape=(768,), dtype=tf.float32, name=\"citation_input\")\n",
    "\n",
    "    case_representation = tf.keras.layers.BatchNormalization()(case_input)\n",
    "    citation_representation = tf.keras.layers.BatchNormalization()(citation_input)\n",
    "\n",
    "    case_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(case_representation)\n",
    "\n",
    "    citation_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(citation_representation)\n",
    "\n",
    "    case_representation = tf.keras.layers.Dropout(dropout,)(case_representation)\n",
    "    citation_representation = tf.keras.layers.Dropout(dropout,)(citation_representation)\n",
    "\n",
    "    case_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(case_representation)\n",
    "    citation_representation = tf.keras.layers.Dense(hidden_dim, activation=\"relu\")(citation_representation)\n",
    "\n",
    "    concatenated = tf.keras.layers.Concatenate()([case_representation, citation_representation])\n",
    "\n",
    "    def shared_stack(prev_input):\n",
    "        return tf.keras.models.Sequential(\n",
    "            [tf.keras.layers.BatchNormalization(),\n",
    "             tf.keras.layers.Dense(hidden_dim, activation=\"relu\"),\n",
    "             tf.keras.layers.Dropout(dropout)\n",
    "            ])(prev_input) \n",
    "    concatenated = shared_stack(concatenated)\n",
    "    concatenated = shared_stack(concatenated)\n",
    "    concatenated = shared_stack(concatenated)\n",
    "    output = tf.keras.layers.Dense(1, dtype=tf.float32, activation=\"sigmoid\")(concatenated)\n",
    "    clf_model = tf.keras.models.Model(inputs=[case_input, citation_input], outputs=[output], name=model_name)\n",
    "    loss= tf.keras.losses.BinaryCrossentropy()\n",
    "    acc = tf.keras.metrics.BinaryAccuracy()\n",
    "    clf_model.compile(loss=loss, metrics=[acc],  optimizer=\"adam\")\n",
    "    clf_model.summary()\n",
    "    return clf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"specter_pooled_clf_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "case_input (InputLayer)         [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "citation_input (InputLayer)     [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 768)          3072        case_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 768)          3072        citation_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          196864      batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          196864      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          65792       dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          65792       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 512)          0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 256)          133376      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 256)          66816       sequential[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 256)          66816       sequential_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 1)            257         sequential_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 798,721\n",
      "Trainable params: 793,601\n",
      "Non-trainable params: 5,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "clf_model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
    "             tf.keras.callbacks.ModelCheckpoint(model_checkpoint, save_best_only=True,),\n",
    "             tf.keras.callbacks.CSVLogger(model_log_fname),\n",
    "             tf.keras.callbacks.ReduceLROnPlateau(patience=3, verbose=1)\n",
    "            ]\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing dataset dict. loading from disk ...\n",
      "loading tensorflow datasets ...\n",
      "batching and shuffling tensorflow datasets ... \n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset, val_dataset, test_dataset = load_clf_datasets(\n",
    "    clf_dataset_dir, data_files, batch_size=128, embeddings_dataset=embeddings_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      " 292/1000 [=======>......................] - ETA: 9:42 - loss: 0.6188 - binary_accuracy: 0.6577"
     ]
    }
   ],
   "source": [
    "history = clf_model.fit(\n",
    "    train_dataset,\n",
    "    steps_per_epoch=1000,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=100,\n",
    "    verbose=1,\n",
    "    epochs=25,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "clf_model.save(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model.evaluate(test_dataset, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf_model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(preds > 0.8).flatten().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.from_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
