{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f693ac",
   "metadata": {},
   "source": [
    "Retrive and rank:\n",
    "    1. Retrieve based on tfidf baseline model replaced by bert and faiss or nmslib\n",
    "    2. Rank and filter the results based on siamese network using legal BERT embeddings.\n",
    "    3. \n",
    "\n",
    "Features:\n",
    "    1. Head matter vs Opinion texts\n",
    "    2. Global vs Local Context\n",
    "    3. By type of Opinion - But not available during test time\n",
    "    4. Date, Jurisdiction, Court ID\n",
    "\n",
    "Models:\n",
    "    1. SPECTRE\n",
    "    2. LEGAL BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f5422ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import datasets\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "raw",
   "id": "hourly-scanner",
   "metadata": {},
   "source": [
    "base_fname = \"../data/subset/case_info.json\"\n",
    "cases_df = pd.read_json(base_fname, lines=True, orient=\"records\")\n",
    "cases_df[\"decision_date\"] = pd.to_datetime(cases_df.decision_date, unit='ms')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "indirect-rabbit",
   "metadata": {},
   "source": [
    "# citations_uids = frozenset(cases_df[\"id\"])\n",
    "# cases_df[\"citation_ids\"] = cases_df.citation_ids.map(lambda x: list(filter(lambda y: y in citations_uids, x)))\n",
    "# cases_df = cases_df[cases_df.citation_ids.map(len) > 0]\n",
    "# cases_df = cases_df.sort_values([\"decision_date\", \"id\"])\n",
    "# cases_df = cases_df.reset_index(drop=True)\n",
    "train_df = cases_df.iloc[:int(len(cases_df) * .8)]\n",
    "test_df = cases_df.loc[cases_df.index.difference(train_df.index)]\n",
    "val_df = test_df.iloc[:int(len(test_df) * .5)]\n",
    "test_df = test_df.loc[test_df.index.difference(val_df.index)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "crazy-society",
   "metadata": {},
   "source": [
    "train_df.to_json(\"../data/subset/train_data.json\", lines=True, orient=\"records\")\n",
    "val_df.to_json(\"../data/subset/val_data.json\", lines=True, orient=\"records\")\n",
    "test_df.to_json(\"../data/subset/test_data.json\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "996ee359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_json(\"../data/subset/train_data.json\", lines=True, orient=\"records\")\n",
    "# val_df = pd.read_json(\"./data/subset/val_data.json\", lines=True, orient=\"records\")\n",
    "# test_df = pd.read_json(\"sample_data.json\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae03123",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetEmbeddingIndex:\n",
    "    def __init__(self, model_name=\"allenai/specter\", exclude_columns=[]):\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = TFAutoModel.from_pretrained(model_name, from_pt=True)\n",
    "        self.tokenizer = partial(self._tokenizer,return_tensors=\"tf\",padding=True, truncation=True, max_length=512)\n",
    "        self.exclude_columns = exclude_columns\n",
    "        self.dataset = None\n",
    "\n",
    "    def load_embeddings(self, batch, inference=False):\n",
    "        if inference:\n",
    "            return self.model(**self.tokenizer(batch))[0][:,0,:].numpy()\n",
    "        return {'embeddings': self.model(**self.tokenizer(batch[\"text\"]))[0][:,0,:].numpy()}\n",
    "\n",
    "    def load_text(self, batch):\n",
    "        return {\"text\": [item[0] + \"\\n\" + item[1] for item in zip(batch['head_matter'], batch['opinion_text'])]}\n",
    "    \n",
    "    def load_dataset(self, data_files):\n",
    "        dataset = datasets.load_dataset(\"json\", data_files=data_files)\n",
    "        dataset = dataset.map(self.load_text, remove_columns=self.exclude_columns, batched=True)\n",
    "        return dataset\n",
    "\n",
    "    \n",
    "    def fit(self, data_files, batch_size=64):\n",
    "        self.dataset = self.load_dataset(data_files)\n",
    "        self.dataset = self.dataset.map(self.load_embeddings, batched=True, batch_size=batch_size)\n",
    "        for k in data_files:\n",
    "            self.dataset[k].add_faiss_index(column='embeddings')\n",
    "        return self\n",
    "    \n",
    "    def save(self, dataset_fname, index_fnames):\n",
    "        for k, v in index_fnames.items():\n",
    "            self.dataset[k].save_faiss_index('embeddings', v)\n",
    "            self.dataset[k].drop_index('embeddings')\n",
    "        self.dataset.save_to_disk(dataset_fname)\n",
    "        return self\n",
    "        \n",
    "    def load(self, dataset_fname, index_fnames):\n",
    "        self.dataset = datasets.load_from_disk(dataset_fname)\n",
    "        for k,v in index_fnames.items():\n",
    "            self.dataset[k].load_faiss_index('embeddings', v)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, queries, dataset_name, top_k=10):\n",
    "        if self.dataset is not None:\n",
    "            query_embeddings = self.load_embeddings(queries, inference=True)\n",
    "            scores, examples = self.dataset[dataset_name].get_nearest_examples_batch('embeddings', query_embeddings, k=top_k)\n",
    "            return scores, examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "different-disposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "embedding_index = DatasetEmbeddingIndex(\n",
    "    exclude_columns = [\"jurisdiction_id\",\"court_id\",\"decision_date\",\n",
    "                       \"head_matter\",\"opinion_text\",\"citation_ids\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-mandate",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ead940c088e772ef\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/61460004/.cache/huggingface/datasets/json/default-ead940c088e772ef/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/61460004/.cache/huggingface/datasets/json/default-ead940c088e772ef/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<bound method DatasetEmbeddingIndex.load_text of <__main__.DatasetEmbeddingIndex object at 0x7f69fcb6b5b0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998a732014184f8f927c256c854c7c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=213.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8099ec18cd0493fb9393fc666dab0b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b3f8e5e3ff425bafcb150cfe591c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=27.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8516ce81ad6e40e788541c54089cd709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3323.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_files = {\n",
    "    \"train\": \"data/subset/train_data.json\",\n",
    "    \"validation\": \"data/subset/val_data.json\",\n",
    "    \"test\": \"data/subset/test_data.json\"}\n",
    "\n",
    "\n",
    "index_fnames = {\n",
    "    \"train\": \"./indices/train_index.fiass\",\n",
    "    \"validation\": \"./indices/val_index.fiass\",\n",
    "    \"test\": \"./indices/test_index.fiass\"\n",
    "}\n",
    "\n",
    "dataset_fname = \"caselaw_dataset\"\n",
    "embedding_index = embedding_index.fit(data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index.save(\"sample_embeddings\", index_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index.load('sample_embeddings', index_fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index.dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-tours",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
